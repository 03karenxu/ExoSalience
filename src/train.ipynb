{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bf66f6",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b660146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747e4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn import CNN1DEncoder, AstronetMVP\n",
    "from dataset import ExoplanetDataset, collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6b11b",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95bf2984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load npz for train / test\n",
    "import numpy as np\n",
    "\n",
    "npz_train = np.load(\"npzs/train.npz\")\n",
    "npz_test = np.load(\"npzs/test.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715c6ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NpzFile 'npzs/train.npz' with keys: global_view, local_view, kepid, label"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d7fc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train / test set from numpy\n",
    "\n",
    "global_train = torch.from_numpy(npz_train['global_view']).float()\n",
    "local_train = torch.from_numpy(npz_train['local_view']).float()\n",
    "labels_train = torch.from_numpy(npz_train['label']).int()\n",
    "ids_train = torch.from_numpy(npz_train['kepid']).float()\n",
    "tab_train = None #torch.from_numpy(npz_train['tabular'])\n",
    "\n",
    "global_test = torch.from_numpy(npz_test['global_view']).float()\n",
    "local_test = torch.from_numpy(npz_test['local_view']).float()\n",
    "labels_test = torch.from_numpy(npz_test['label']).int()\n",
    "ids_test = torch.from_numpy(npz_test['kepid']).float()\n",
    "tab_test = None #torch.from_numpy(npz_test['tabular'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b52a0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 1, 0, 2, 0, 2, 1, 1,\n",
       "        0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 2, 0, 0, 2, 0, 0,\n",
       "        0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 2, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd898d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "2 \n",
      "\n",
      "2\n",
      "torch.Size([72, 201]) \n",
      "\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(global_train.shape[1])\n",
    "print(global_test.ndim, \"\\n\")\n",
    "\n",
    "print(local_train.ndim)\n",
    "print(local_test.shape, \"\\n\")\n",
    "\n",
    "print(labels_train.ndim)\n",
    "print(labels_test.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1f89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ExoplanetDataset(global_train, local_train, labels_train, ids_train, tabular=tab_train)\n",
    "ds_test  = ExoplanetDataset(global_test,  local_test,  labels_test,  ids_test,  tabular=tab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3327c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders from train / test data sets\n",
    "dl_train = DataLoader(ds_train, batch_size = 64, shuffle=True, collate_fn=collate, drop_last=False)\n",
    "dl_test = DataLoader(ds_test, batch_size = 64, shuffle=True, collate_fn=collate, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83c727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer, pos weight from train set\n",
    "def compute_pos_weight_from_loader(dloader):\n",
    "    pos = neg = 0\n",
    "    with torch.no_grad():\n",
    "        for _xg, _xl, _xt, _y, _ids in dloader:\n",
    "            pos += (_y == 1).sum().item()\n",
    "            neg += (_y == 0).sum().item()\n",
    "    if pos == 0 or neg == 0: return None\n",
    "    return torch.tensor([neg / max(pos, 1.0)], dtype=torch.float32, device=device)\n",
    "\n",
    "# Infer tabular_dim from one train batch\n",
    "xg0, xl0, xt0, y0, ids0 = next(iter(dl_train))\n",
    "tabular_dim = 0 if xt0 is None else xt0.shape[1]\n",
    "\n",
    "model = AstronetMVP(hidden=64, k_global=7, k_local=5, tabular_dim=tabular_dim).to(device)\n",
    "\n",
    "pos_weight = compute_pos_weight_from_loader(dl_train)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81084630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make & save config / checkpoints dir\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# make & save config\n",
    "config = {\n",
    "    \"hidden\":64,\n",
    "    \"k_global\": 7,\n",
    "    \"k_local\": 5,\n",
    "    \"tabular_dim\": tabular_dim,\n",
    "    \"lr\": 1e-3,\n",
    "    \"wd\": 1e-5,\n",
    "    \"pos_weight\": None if pos_weight is None else float(pos_weight.item())\n",
    "}\n",
    "json.dump(config, open(os.path.join(save_dir, \"config.json\"), \"w\"), indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "861f48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dloader, optim, criterion):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "\n",
    "    # feed train data\n",
    "    for xg, xl, xt, y, _ids in dloader:\n",
    "\n",
    "        # move to gpu\n",
    "        xg = xg.to(device)\n",
    "        xl = xl.to(device)\n",
    "        y = y.to(device)# expected\n",
    "\n",
    "        # feed\n",
    "        xt = None if xt is None else xt.to(device)\n",
    "        logits = model(xg, xl, xt).squeeze(1) # actual\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        #compute optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total += loss.item() * bs\n",
    "        n+= bs\n",
    "\n",
    "    return total / max(n,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "454b9498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58703/477851736.py:21: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  nn.utils.clip_grad_norm(model.parameters(), max_norm=5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/1000 | train_loss=1.0256 | 0.0s\n",
      "Epoch 02/1000 | train_loss=1.0230 | 0.0s\n",
      "Epoch 03/1000 | train_loss=1.0200 | 0.0s\n",
      "Epoch 04/1000 | train_loss=1.0183 | 0.0s\n",
      "Epoch 05/1000 | train_loss=1.0159 | 0.0s\n",
      "Epoch 06/1000 | train_loss=1.0117 | 0.0s\n",
      "Epoch 07/1000 | train_loss=1.0096 | 0.0s\n",
      "Epoch 08/1000 | train_loss=1.0068 | 0.0s\n",
      "Epoch 09/1000 | train_loss=1.0057 | 0.0s\n",
      "Epoch 10/1000 | train_loss=1.0051 | 0.0s\n",
      "Epoch 11/1000 | train_loss=1.0011 | 0.0s\n",
      "Epoch 12/1000 | train_loss=0.9953 | 0.0s\n",
      "Epoch 13/1000 | train_loss=0.9977 | 0.0s\n",
      "Epoch 14/1000 | train_loss=0.9910 | 0.0s\n",
      "Epoch 15/1000 | train_loss=0.9896 | 0.0s\n",
      "Epoch 16/1000 | train_loss=0.9867 | 0.0s\n",
      "Epoch 17/1000 | train_loss=0.9837 | 0.0s\n",
      "Epoch 18/1000 | train_loss=0.9823 | 0.0s\n",
      "Epoch 19/1000 | train_loss=0.9841 | 0.0s\n",
      "Epoch 20/1000 | train_loss=0.9810 | 0.0s\n",
      "Epoch 21/1000 | train_loss=0.9771 | 0.0s\n",
      "Epoch 22/1000 | train_loss=0.9758 | 0.0s\n",
      "Epoch 23/1000 | train_loss=0.9749 | 0.0s\n",
      "Epoch 24/1000 | train_loss=0.9728 | 0.0s\n",
      "Epoch 25/1000 | train_loss=0.9694 | 0.0s\n",
      "Epoch 26/1000 | train_loss=0.9674 | 0.0s\n",
      "Epoch 27/1000 | train_loss=0.9653 | 0.0s\n",
      "Epoch 28/1000 | train_loss=0.9616 | 0.0s\n",
      "Epoch 29/1000 | train_loss=0.9627 | 0.0s\n",
      "Epoch 30/1000 | train_loss=0.9615 | 0.0s\n",
      "Epoch 31/1000 | train_loss=0.9593 | 0.0s\n",
      "Epoch 32/1000 | train_loss=0.9586 | 0.0s\n",
      "Epoch 33/1000 | train_loss=0.9557 | 0.0s\n",
      "Epoch 34/1000 | train_loss=0.9515 | 0.0s\n",
      "Epoch 35/1000 | train_loss=0.9498 | 0.0s\n",
      "Epoch 36/1000 | train_loss=0.9478 | 0.0s\n",
      "Epoch 37/1000 | train_loss=0.9465 | 0.0s\n",
      "Epoch 38/1000 | train_loss=0.9426 | 0.0s\n",
      "Epoch 39/1000 | train_loss=0.9483 | 0.0s\n",
      "Epoch 40/1000 | train_loss=0.9410 | 0.0s\n",
      "Epoch 41/1000 | train_loss=0.9400 | 0.0s\n",
      "Epoch 42/1000 | train_loss=0.9368 | 0.0s\n",
      "Epoch 43/1000 | train_loss=0.9322 | 0.0s\n",
      "Epoch 44/1000 | train_loss=0.9324 | 0.0s\n",
      "Epoch 45/1000 | train_loss=0.9302 | 0.0s\n",
      "Epoch 46/1000 | train_loss=0.9284 | 0.0s\n",
      "Epoch 47/1000 | train_loss=0.9260 | 0.0s\n",
      "Epoch 48/1000 | train_loss=0.9220 | 0.0s\n",
      "Epoch 49/1000 | train_loss=0.9232 | 0.0s\n",
      "Epoch 50/1000 | train_loss=0.9209 | 0.0s\n",
      "Epoch 51/1000 | train_loss=0.9187 | 0.0s\n",
      "Epoch 52/1000 | train_loss=0.9139 | 0.0s\n",
      "Epoch 53/1000 | train_loss=0.9207 | 0.0s\n",
      "Epoch 54/1000 | train_loss=0.9149 | 0.0s\n",
      "Epoch 55/1000 | train_loss=0.9075 | 0.0s\n",
      "Epoch 56/1000 | train_loss=0.9061 | 0.0s\n",
      "Epoch 57/1000 | train_loss=0.9066 | 0.0s\n",
      "Epoch 58/1000 | train_loss=0.8995 | 0.0s\n",
      "Epoch 59/1000 | train_loss=0.8992 | 0.0s\n",
      "Epoch 60/1000 | train_loss=0.8950 | 0.0s\n",
      "Epoch 61/1000 | train_loss=0.8930 | 0.0s\n",
      "Epoch 62/1000 | train_loss=0.8922 | 0.0s\n",
      "Epoch 63/1000 | train_loss=0.8867 | 0.0s\n",
      "Epoch 64/1000 | train_loss=0.8890 | 0.0s\n",
      "Epoch 65/1000 | train_loss=0.8841 | 0.0s\n",
      "Epoch 66/1000 | train_loss=0.8911 | 0.0s\n",
      "Epoch 67/1000 | train_loss=0.8787 | 0.0s\n",
      "Epoch 68/1000 | train_loss=0.8792 | 0.0s\n",
      "Epoch 69/1000 | train_loss=0.8704 | 0.0s\n",
      "Epoch 70/1000 | train_loss=0.8680 | 0.0s\n",
      "Epoch 71/1000 | train_loss=0.8729 | 0.0s\n",
      "Epoch 72/1000 | train_loss=0.8687 | 0.0s\n",
      "Epoch 73/1000 | train_loss=0.8624 | 0.0s\n",
      "Epoch 74/1000 | train_loss=0.8585 | 0.0s\n",
      "Epoch 75/1000 | train_loss=0.8537 | 0.0s\n",
      "Epoch 76/1000 | train_loss=0.8517 | 0.0s\n",
      "Epoch 77/1000 | train_loss=0.8513 | 0.0s\n",
      "Epoch 78/1000 | train_loss=0.8509 | 0.0s\n",
      "Epoch 79/1000 | train_loss=0.8441 | 0.0s\n",
      "Epoch 80/1000 | train_loss=0.8473 | 0.0s\n",
      "Epoch 81/1000 | train_loss=0.8415 | 0.0s\n",
      "Epoch 82/1000 | train_loss=0.8331 | 0.0s\n",
      "Epoch 83/1000 | train_loss=0.8294 | 0.0s\n",
      "Epoch 84/1000 | train_loss=0.8268 | 0.0s\n",
      "Epoch 85/1000 | train_loss=0.8192 | 0.0s\n",
      "Epoch 86/1000 | train_loss=0.8239 | 0.0s\n",
      "Epoch 87/1000 | train_loss=0.8252 | 0.0s\n",
      "Epoch 88/1000 | train_loss=0.8226 | 0.0s\n",
      "Epoch 89/1000 | train_loss=0.8072 | 0.0s\n",
      "Epoch 90/1000 | train_loss=0.8059 | 0.0s\n",
      "Epoch 91/1000 | train_loss=0.8066 | 0.0s\n",
      "Epoch 92/1000 | train_loss=0.7921 | 0.0s\n",
      "Epoch 93/1000 | train_loss=0.7999 | 0.0s\n",
      "Epoch 94/1000 | train_loss=0.7976 | 0.0s\n",
      "Epoch 95/1000 | train_loss=0.7882 | 0.0s\n",
      "Epoch 96/1000 | train_loss=0.7804 | 0.0s\n",
      "Epoch 97/1000 | train_loss=0.7811 | 0.0s\n",
      "Epoch 98/1000 | train_loss=0.7764 | 0.0s\n",
      "Epoch 99/1000 | train_loss=0.7712 | 0.0s\n",
      "Epoch 100/1000 | train_loss=0.7686 | 0.0s\n",
      "Epoch 101/1000 | train_loss=0.7754 | 0.0s\n",
      "Epoch 102/1000 | train_loss=0.7780 | 0.0s\n",
      "Epoch 103/1000 | train_loss=0.7568 | 0.0s\n",
      "Epoch 104/1000 | train_loss=0.7527 | 0.0s\n",
      "Epoch 105/1000 | train_loss=0.7500 | 0.0s\n",
      "Epoch 106/1000 | train_loss=0.7473 | 0.0s\n",
      "Epoch 107/1000 | train_loss=0.7427 | 0.0s\n",
      "Epoch 108/1000 | train_loss=0.7382 | 0.0s\n",
      "Epoch 109/1000 | train_loss=0.7330 | 0.0s\n",
      "Epoch 110/1000 | train_loss=0.7291 | 0.0s\n",
      "Epoch 111/1000 | train_loss=0.7334 | 0.0s\n",
      "Epoch 112/1000 | train_loss=0.7252 | 0.0s\n",
      "Epoch 113/1000 | train_loss=0.7164 | 0.0s\n",
      "Epoch 114/1000 | train_loss=0.7172 | 0.0s\n",
      "Epoch 115/1000 | train_loss=0.7114 | 0.0s\n",
      "Epoch 116/1000 | train_loss=0.7121 | 0.0s\n",
      "Epoch 117/1000 | train_loss=0.7089 | 0.0s\n",
      "Epoch 118/1000 | train_loss=0.7035 | 0.0s\n",
      "Epoch 119/1000 | train_loss=0.6960 | 0.0s\n",
      "Epoch 120/1000 | train_loss=0.6985 | 0.0s\n",
      "Epoch 121/1000 | train_loss=0.6845 | 0.0s\n",
      "Epoch 122/1000 | train_loss=0.7120 | 0.0s\n",
      "Epoch 123/1000 | train_loss=0.6843 | 0.0s\n",
      "Epoch 124/1000 | train_loss=0.6879 | 0.0s\n",
      "Epoch 125/1000 | train_loss=0.6814 | 0.0s\n",
      "Epoch 126/1000 | train_loss=0.6664 | 0.0s\n",
      "Epoch 127/1000 | train_loss=0.6702 | 0.0s\n",
      "Epoch 128/1000 | train_loss=0.6648 | 0.0s\n",
      "Epoch 129/1000 | train_loss=0.6563 | 0.0s\n",
      "Epoch 130/1000 | train_loss=0.6545 | 0.0s\n",
      "Epoch 131/1000 | train_loss=0.6541 | 0.0s\n",
      "Epoch 132/1000 | train_loss=0.6488 | 0.0s\n",
      "Epoch 133/1000 | train_loss=0.6407 | 0.0s\n",
      "Epoch 134/1000 | train_loss=0.6380 | 0.0s\n",
      "Epoch 135/1000 | train_loss=0.6333 | 0.0s\n",
      "Epoch 136/1000 | train_loss=0.6329 | 0.0s\n",
      "Epoch 137/1000 | train_loss=0.6300 | 0.0s\n",
      "Epoch 138/1000 | train_loss=0.6307 | 0.0s\n",
      "Epoch 139/1000 | train_loss=0.6131 | 0.0s\n",
      "Epoch 140/1000 | train_loss=0.6262 | 0.0s\n",
      "Epoch 141/1000 | train_loss=0.6291 | 0.0s\n",
      "Epoch 142/1000 | train_loss=0.6065 | 0.0s\n",
      "Epoch 143/1000 | train_loss=0.6043 | 0.0s\n",
      "Epoch 144/1000 | train_loss=0.5996 | 0.0s\n",
      "Epoch 145/1000 | train_loss=0.5943 | 0.0s\n",
      "Epoch 146/1000 | train_loss=0.5922 | 0.0s\n",
      "Epoch 147/1000 | train_loss=0.5892 | 0.0s\n",
      "Epoch 148/1000 | train_loss=0.5857 | 0.0s\n",
      "Epoch 149/1000 | train_loss=0.5791 | 0.0s\n",
      "Epoch 150/1000 | train_loss=0.5843 | 0.0s\n",
      "Epoch 151/1000 | train_loss=0.5725 | 0.0s\n",
      "Epoch 152/1000 | train_loss=0.5756 | 0.0s\n",
      "Epoch 153/1000 | train_loss=0.5764 | 0.0s\n",
      "Epoch 154/1000 | train_loss=0.5638 | 0.0s\n",
      "Epoch 155/1000 | train_loss=0.5648 | 0.0s\n",
      "Epoch 156/1000 | train_loss=0.5801 | 0.0s\n",
      "Epoch 157/1000 | train_loss=0.5800 | 0.0s\n",
      "Epoch 158/1000 | train_loss=0.5536 | 0.0s\n",
      "Epoch 159/1000 | train_loss=0.5430 | 0.0s\n",
      "Epoch 160/1000 | train_loss=0.5392 | 0.0s\n",
      "Epoch 161/1000 | train_loss=0.5418 | 0.0s\n",
      "Epoch 162/1000 | train_loss=0.5347 | 0.0s\n",
      "Epoch 163/1000 | train_loss=0.5504 | 0.0s\n",
      "Epoch 164/1000 | train_loss=0.5275 | 0.0s\n",
      "Epoch 165/1000 | train_loss=0.5250 | 0.0s\n",
      "Epoch 166/1000 | train_loss=0.5116 | 0.0s\n",
      "Epoch 167/1000 | train_loss=0.5437 | 0.0s\n",
      "Epoch 168/1000 | train_loss=0.5105 | 0.0s\n",
      "Epoch 169/1000 | train_loss=0.5119 | 0.0s\n",
      "Epoch 170/1000 | train_loss=0.5059 | 0.0s\n",
      "Epoch 171/1000 | train_loss=0.5006 | 0.0s\n",
      "Epoch 172/1000 | train_loss=0.4972 | 0.0s\n",
      "Epoch 173/1000 | train_loss=0.5081 | 0.0s\n",
      "Epoch 174/1000 | train_loss=0.5181 | 0.0s\n",
      "Epoch 175/1000 | train_loss=0.5040 | 0.0s\n",
      "Epoch 176/1000 | train_loss=0.4892 | 0.0s\n",
      "Epoch 177/1000 | train_loss=0.4925 | 0.0s\n",
      "Epoch 178/1000 | train_loss=0.4752 | 0.0s\n",
      "Epoch 179/1000 | train_loss=0.4767 | 0.0s\n",
      "Epoch 180/1000 | train_loss=0.4687 | 0.0s\n",
      "Epoch 181/1000 | train_loss=0.4752 | 0.0s\n",
      "Epoch 182/1000 | train_loss=0.4514 | 0.0s\n",
      "Epoch 183/1000 | train_loss=0.4665 | 0.0s\n",
      "Epoch 184/1000 | train_loss=0.4528 | 0.0s\n",
      "Epoch 185/1000 | train_loss=0.4499 | 0.0s\n",
      "Epoch 186/1000 | train_loss=0.4437 | 0.0s\n",
      "Epoch 187/1000 | train_loss=0.4504 | 0.0s\n",
      "Epoch 188/1000 | train_loss=0.4399 | 0.0s\n",
      "Epoch 189/1000 | train_loss=0.4568 | 0.0s\n",
      "Epoch 190/1000 | train_loss=0.4335 | 0.0s\n",
      "Epoch 191/1000 | train_loss=0.4262 | 0.0s\n",
      "Epoch 192/1000 | train_loss=0.4255 | 0.0s\n",
      "Epoch 193/1000 | train_loss=0.4308 | 0.0s\n",
      "Epoch 194/1000 | train_loss=0.4307 | 0.0s\n",
      "Epoch 195/1000 | train_loss=0.4175 | 0.0s\n",
      "Epoch 196/1000 | train_loss=0.4211 | 0.0s\n",
      "Epoch 197/1000 | train_loss=0.4150 | 0.0s\n",
      "Epoch 198/1000 | train_loss=0.4116 | 0.0s\n",
      "Epoch 199/1000 | train_loss=0.4111 | 0.0s\n",
      "Epoch 200/1000 | train_loss=0.3884 | 0.0s\n",
      "Epoch 201/1000 | train_loss=0.4345 | 0.0s\n",
      "Epoch 202/1000 | train_loss=0.3963 | 0.0s\n",
      "Epoch 203/1000 | train_loss=0.4130 | 0.0s\n",
      "Epoch 204/1000 | train_loss=0.3836 | 0.0s\n",
      "Epoch 205/1000 | train_loss=0.4111 | 0.0s\n",
      "Epoch 206/1000 | train_loss=0.3822 | 0.0s\n",
      "Epoch 207/1000 | train_loss=0.3858 | 0.0s\n",
      "Epoch 208/1000 | train_loss=0.3753 | 0.0s\n",
      "Epoch 209/1000 | train_loss=0.3696 | 0.0s\n",
      "Epoch 210/1000 | train_loss=0.3607 | 0.0s\n",
      "Epoch 211/1000 | train_loss=0.3667 | 0.0s\n",
      "Epoch 212/1000 | train_loss=0.3534 | 0.0s\n",
      "Epoch 213/1000 | train_loss=0.3578 | 0.0s\n",
      "Epoch 214/1000 | train_loss=0.3514 | 0.0s\n",
      "Epoch 215/1000 | train_loss=0.3505 | 0.0s\n",
      "Epoch 216/1000 | train_loss=0.3395 | 0.0s\n",
      "Epoch 217/1000 | train_loss=0.3588 | 0.0s\n",
      "Epoch 218/1000 | train_loss=0.3484 | 0.0s\n",
      "Epoch 219/1000 | train_loss=0.3457 | 0.0s\n",
      "Epoch 220/1000 | train_loss=0.3289 | 0.0s\n",
      "Epoch 221/1000 | train_loss=0.3329 | 0.0s\n",
      "Epoch 222/1000 | train_loss=0.3347 | 0.0s\n",
      "Epoch 223/1000 | train_loss=0.3350 | 0.0s\n",
      "Epoch 224/1000 | train_loss=0.3228 | 0.0s\n",
      "Epoch 225/1000 | train_loss=0.3196 | 0.0s\n",
      "Epoch 226/1000 | train_loss=0.3101 | 0.0s\n",
      "Epoch 227/1000 | train_loss=0.3086 | 0.0s\n",
      "Epoch 228/1000 | train_loss=0.3060 | 0.0s\n",
      "Epoch 229/1000 | train_loss=0.3005 | 0.0s\n",
      "Epoch 230/1000 | train_loss=0.3243 | 0.0s\n",
      "Epoch 231/1000 | train_loss=0.3178 | 0.0s\n",
      "Epoch 232/1000 | train_loss=0.2970 | 0.0s\n",
      "Epoch 233/1000 | train_loss=0.2990 | 0.0s\n",
      "Epoch 234/1000 | train_loss=0.2995 | 0.0s\n",
      "Epoch 235/1000 | train_loss=0.2811 | 0.0s\n",
      "Epoch 236/1000 | train_loss=0.2946 | 0.0s\n",
      "Epoch 237/1000 | train_loss=0.2855 | 0.0s\n",
      "Epoch 238/1000 | train_loss=0.2808 | 0.0s\n",
      "Epoch 239/1000 | train_loss=0.2727 | 0.0s\n",
      "Epoch 240/1000 | train_loss=0.2587 | 0.0s\n",
      "Epoch 241/1000 | train_loss=0.2694 | 0.0s\n",
      "Epoch 242/1000 | train_loss=0.3109 | 0.0s\n",
      "Epoch 243/1000 | train_loss=0.2774 | 0.0s\n",
      "Epoch 244/1000 | train_loss=0.2957 | 0.0s\n",
      "Epoch 245/1000 | train_loss=0.2697 | 0.0s\n",
      "Epoch 246/1000 | train_loss=0.2608 | 0.0s\n",
      "Epoch 247/1000 | train_loss=0.2665 | 0.0s\n",
      "Epoch 248/1000 | train_loss=0.2576 | 0.0s\n",
      "Epoch 249/1000 | train_loss=0.2723 | 0.0s\n",
      "Epoch 250/1000 | train_loss=0.2411 | 0.0s\n",
      "Epoch 251/1000 | train_loss=0.2607 | 0.0s\n",
      "Epoch 252/1000 | train_loss=0.2329 | 0.0s\n",
      "Epoch 253/1000 | train_loss=0.2355 | 0.0s\n",
      "Epoch 254/1000 | train_loss=0.2277 | 0.0s\n",
      "Epoch 255/1000 | train_loss=0.2351 | 0.0s\n",
      "Epoch 256/1000 | train_loss=0.2383 | 0.0s\n",
      "Epoch 257/1000 | train_loss=0.2313 | 0.0s\n",
      "Epoch 258/1000 | train_loss=0.2077 | 0.0s\n",
      "Epoch 259/1000 | train_loss=0.2541 | 0.0s\n",
      "Epoch 260/1000 | train_loss=0.2184 | 0.0s\n",
      "Epoch 261/1000 | train_loss=0.2271 | 0.0s\n",
      "Epoch 262/1000 | train_loss=0.1974 | 0.0s\n",
      "Epoch 263/1000 | train_loss=0.2052 | 0.0s\n",
      "Epoch 264/1000 | train_loss=0.1993 | 0.0s\n",
      "Epoch 265/1000 | train_loss=0.1947 | 0.0s\n",
      "Epoch 266/1000 | train_loss=0.1988 | 0.0s\n",
      "Epoch 267/1000 | train_loss=0.2026 | 0.0s\n",
      "Epoch 268/1000 | train_loss=0.1935 | 0.0s\n",
      "Epoch 269/1000 | train_loss=0.1907 | 0.0s\n",
      "Epoch 270/1000 | train_loss=0.1782 | 0.0s\n",
      "Epoch 271/1000 | train_loss=0.1780 | 0.0s\n",
      "Epoch 272/1000 | train_loss=0.1672 | 0.0s\n",
      "Epoch 273/1000 | train_loss=0.1751 | 0.0s\n",
      "Epoch 274/1000 | train_loss=0.1702 | 0.0s\n",
      "Epoch 275/1000 | train_loss=0.1701 | 0.0s\n",
      "Epoch 276/1000 | train_loss=0.1735 | 0.0s\n",
      "Epoch 277/1000 | train_loss=0.1715 | 0.0s\n",
      "Epoch 278/1000 | train_loss=0.1658 | 0.0s\n",
      "Epoch 279/1000 | train_loss=0.1766 | 0.0s\n",
      "Epoch 280/1000 | train_loss=0.1481 | 0.0s\n",
      "Epoch 281/1000 | train_loss=0.2005 | 0.0s\n",
      "Epoch 282/1000 | train_loss=0.1700 | 0.0s\n",
      "Epoch 283/1000 | train_loss=0.1651 | 0.0s\n",
      "Epoch 284/1000 | train_loss=0.1717 | 0.0s\n",
      "Epoch 285/1000 | train_loss=0.1550 | 0.0s\n",
      "Epoch 286/1000 | train_loss=0.1341 | 0.0s\n",
      "Epoch 287/1000 | train_loss=0.1520 | 0.0s\n",
      "Epoch 288/1000 | train_loss=0.1262 | 0.0s\n",
      "Epoch 289/1000 | train_loss=0.1205 | 0.0s\n",
      "Epoch 290/1000 | train_loss=0.1259 | 0.0s\n",
      "Epoch 291/1000 | train_loss=0.1215 | 0.0s\n",
      "Epoch 292/1000 | train_loss=0.1227 | 0.0s\n",
      "Epoch 293/1000 | train_loss=0.1320 | 0.0s\n",
      "Epoch 294/1000 | train_loss=0.1128 | 0.0s\n",
      "Epoch 295/1000 | train_loss=0.1104 | 0.0s\n",
      "Epoch 296/1000 | train_loss=0.1271 | 0.0s\n",
      "Epoch 297/1000 | train_loss=0.1087 | 0.0s\n",
      "Epoch 298/1000 | train_loss=0.1028 | 0.0s\n",
      "Epoch 299/1000 | train_loss=0.1095 | 0.0s\n",
      "Epoch 300/1000 | train_loss=0.0995 | 0.0s\n",
      "Epoch 301/1000 | train_loss=0.0934 | 0.0s\n",
      "Epoch 302/1000 | train_loss=0.0933 | 0.0s\n",
      "Epoch 303/1000 | train_loss=0.1012 | 0.0s\n",
      "Epoch 304/1000 | train_loss=0.0853 | 0.0s\n",
      "Epoch 305/1000 | train_loss=0.0830 | 0.0s\n",
      "Epoch 306/1000 | train_loss=0.0938 | 0.0s\n",
      "Epoch 307/1000 | train_loss=0.0721 | 0.0s\n",
      "Epoch 308/1000 | train_loss=0.0718 | 0.0s\n",
      "Epoch 309/1000 | train_loss=0.0660 | 0.0s\n",
      "Epoch 310/1000 | train_loss=0.0737 | 0.0s\n",
      "Epoch 311/1000 | train_loss=0.0663 | 0.0s\n",
      "Epoch 312/1000 | train_loss=0.0630 | 0.0s\n",
      "Epoch 313/1000 | train_loss=0.0713 | 0.0s\n",
      "Epoch 314/1000 | train_loss=0.0834 | 0.0s\n",
      "Epoch 315/1000 | train_loss=0.0710 | 0.0s\n",
      "Epoch 316/1000 | train_loss=0.0925 | 0.0s\n",
      "Epoch 317/1000 | train_loss=0.0434 | 0.0s\n",
      "Epoch 318/1000 | train_loss=0.0458 | 0.0s\n",
      "Epoch 319/1000 | train_loss=0.0329 | 0.0s\n",
      "Epoch 320/1000 | train_loss=0.0408 | 0.0s\n",
      "Epoch 321/1000 | train_loss=0.0408 | 0.0s\n",
      "Epoch 322/1000 | train_loss=0.0430 | 0.0s\n",
      "Epoch 323/1000 | train_loss=0.0185 | 0.0s\n",
      "Epoch 324/1000 | train_loss=0.0323 | 0.0s\n",
      "Epoch 325/1000 | train_loss=0.0458 | 0.0s\n",
      "Epoch 326/1000 | train_loss=0.0379 | 0.0s\n",
      "Epoch 327/1000 | train_loss=0.0092 | 0.0s\n",
      "Epoch 328/1000 | train_loss=0.0071 | 0.0s\n",
      "Epoch 329/1000 | train_loss=0.0107 | 0.0s\n",
      "Epoch 330/1000 | train_loss=0.0366 | 0.0s\n",
      "Epoch 331/1000 | train_loss=0.0335 | 0.0s\n",
      "Epoch 332/1000 | train_loss=0.0545 | 0.0s\n",
      "Epoch 333/1000 | train_loss=0.0261 | 0.0s\n",
      "Epoch 334/1000 | train_loss=0.0864 | 0.0s\n",
      "Epoch 335/1000 | train_loss=0.0935 | 0.0s\n",
      "Epoch 336/1000 | train_loss=0.0353 | 0.0s\n",
      "Epoch 337/1000 | train_loss=0.0366 | 0.0s\n",
      "Epoch 338/1000 | train_loss=-0.0288 | 0.0s\n",
      "Epoch 339/1000 | train_loss=-0.0000 | 0.0s\n",
      "Epoch 340/1000 | train_loss=-0.0217 | 0.0s\n",
      "Epoch 341/1000 | train_loss=-0.0173 | 0.0s\n",
      "Epoch 342/1000 | train_loss=-0.0354 | 0.0s\n",
      "Epoch 343/1000 | train_loss=0.0292 | 0.0s\n",
      "Epoch 344/1000 | train_loss=0.0086 | 0.0s\n",
      "Epoch 345/1000 | train_loss=-0.0334 | 0.0s\n",
      "Epoch 346/1000 | train_loss=-0.0341 | 0.0s\n",
      "Epoch 347/1000 | train_loss=-0.0433 | 0.0s\n",
      "Epoch 348/1000 | train_loss=-0.0444 | 0.0s\n",
      "Epoch 349/1000 | train_loss=-0.0531 | 0.0s\n",
      "Epoch 350/1000 | train_loss=-0.0546 | 0.0s\n",
      "Epoch 351/1000 | train_loss=-0.0566 | 0.0s\n",
      "Epoch 352/1000 | train_loss=-0.0591 | 0.0s\n",
      "Epoch 353/1000 | train_loss=-0.0662 | 0.0s\n",
      "Epoch 354/1000 | train_loss=-0.0736 | 0.0s\n",
      "Epoch 355/1000 | train_loss=-0.0335 | 0.0s\n",
      "Epoch 356/1000 | train_loss=-0.0541 | 0.0s\n",
      "Epoch 357/1000 | train_loss=-0.0584 | 0.0s\n",
      "Epoch 358/1000 | train_loss=-0.0721 | 0.0s\n",
      "Epoch 359/1000 | train_loss=-0.0638 | 0.0s\n",
      "Epoch 360/1000 | train_loss=-0.0335 | 0.0s\n",
      "Epoch 361/1000 | train_loss=-0.0721 | 0.0s\n",
      "Epoch 362/1000 | train_loss=-0.0503 | 0.0s\n",
      "Epoch 363/1000 | train_loss=-0.0479 | 0.0s\n",
      "Epoch 364/1000 | train_loss=-0.0932 | 0.0s\n",
      "Epoch 365/1000 | train_loss=-0.0893 | 0.0s\n",
      "Epoch 366/1000 | train_loss=-0.1032 | 0.0s\n",
      "Epoch 367/1000 | train_loss=-0.0690 | 0.0s\n",
      "Epoch 368/1000 | train_loss=-0.0937 | 0.0s\n",
      "Epoch 369/1000 | train_loss=-0.0842 | 0.0s\n",
      "Epoch 370/1000 | train_loss=-0.1130 | 0.0s\n",
      "Epoch 371/1000 | train_loss=-0.0972 | 0.0s\n",
      "Epoch 372/1000 | train_loss=-0.1087 | 0.0s\n",
      "Epoch 373/1000 | train_loss=-0.1023 | 0.0s\n",
      "Epoch 374/1000 | train_loss=-0.1014 | 0.0s\n",
      "Epoch 375/1000 | train_loss=-0.1146 | 0.0s\n",
      "Epoch 376/1000 | train_loss=-0.1282 | 0.0s\n",
      "Epoch 377/1000 | train_loss=-0.1347 | 0.0s\n",
      "Epoch 378/1000 | train_loss=-0.0952 | 0.0s\n",
      "Epoch 379/1000 | train_loss=-0.1267 | 0.0s\n",
      "Epoch 380/1000 | train_loss=-0.1195 | 0.0s\n",
      "Epoch 381/1000 | train_loss=-0.1527 | 0.0s\n",
      "Epoch 382/1000 | train_loss=-0.1112 | 0.0s\n",
      "Epoch 383/1000 | train_loss=-0.1461 | 0.0s\n",
      "Epoch 384/1000 | train_loss=-0.1271 | 0.0s\n",
      "Epoch 385/1000 | train_loss=-0.1546 | 0.0s\n",
      "Epoch 386/1000 | train_loss=-0.1459 | 0.0s\n",
      "Epoch 387/1000 | train_loss=-0.1574 | 0.0s\n",
      "Epoch 388/1000 | train_loss=-0.1728 | 0.0s\n",
      "Epoch 389/1000 | train_loss=-0.1768 | 0.0s\n",
      "Epoch 390/1000 | train_loss=-0.1542 | 0.0s\n",
      "Epoch 391/1000 | train_loss=-0.1618 | 0.0s\n",
      "Epoch 392/1000 | train_loss=-0.1691 | 0.0s\n",
      "Epoch 393/1000 | train_loss=-0.1705 | 0.0s\n",
      "Epoch 394/1000 | train_loss=-0.1527 | 0.0s\n",
      "Epoch 395/1000 | train_loss=-0.1787 | 0.0s\n",
      "Epoch 396/1000 | train_loss=-0.1799 | 0.0s\n",
      "Epoch 397/1000 | train_loss=-0.1852 | 0.0s\n",
      "Epoch 398/1000 | train_loss=-0.1953 | 0.0s\n",
      "Epoch 399/1000 | train_loss=-0.1939 | 0.0s\n",
      "Epoch 400/1000 | train_loss=-0.1938 | 0.0s\n",
      "Epoch 401/1000 | train_loss=-0.1887 | 0.0s\n",
      "Epoch 402/1000 | train_loss=-0.2019 | 0.0s\n",
      "Epoch 403/1000 | train_loss=-0.1934 | 0.0s\n",
      "Epoch 404/1000 | train_loss=-0.2090 | 0.0s\n",
      "Epoch 405/1000 | train_loss=-0.1918 | 0.0s\n",
      "Epoch 406/1000 | train_loss=-0.2051 | 0.0s\n",
      "Epoch 407/1000 | train_loss=-0.1996 | 0.0s\n",
      "Epoch 408/1000 | train_loss=-0.2204 | 0.0s\n",
      "Epoch 409/1000 | train_loss=-0.2221 | 0.0s\n",
      "Epoch 410/1000 | train_loss=-0.2074 | 0.0s\n",
      "Epoch 411/1000 | train_loss=-0.2121 | 0.0s\n",
      "Epoch 412/1000 | train_loss=-0.2227 | 0.0s\n",
      "Epoch 413/1000 | train_loss=-0.2358 | 0.0s\n",
      "Epoch 414/1000 | train_loss=-0.2323 | 0.0s\n",
      "Epoch 415/1000 | train_loss=-0.2262 | 0.0s\n",
      "Epoch 416/1000 | train_loss=-0.2288 | 0.0s\n",
      "Epoch 417/1000 | train_loss=-0.2376 | 0.0s\n",
      "Epoch 418/1000 | train_loss=-0.2602 | 0.0s\n",
      "Epoch 419/1000 | train_loss=-0.2324 | 0.0s\n",
      "Epoch 420/1000 | train_loss=-0.2439 | 0.0s\n",
      "Epoch 421/1000 | train_loss=-0.2456 | 0.0s\n",
      "Epoch 422/1000 | train_loss=-0.2516 | 0.0s\n",
      "Epoch 423/1000 | train_loss=-0.2365 | 0.0s\n",
      "Epoch 424/1000 | train_loss=-0.2573 | 0.0s\n",
      "Epoch 425/1000 | train_loss=-0.2634 | 0.0s\n",
      "Epoch 426/1000 | train_loss=-0.2706 | 0.0s\n",
      "Epoch 427/1000 | train_loss=-0.2730 | 0.0s\n",
      "Epoch 428/1000 | train_loss=-0.2653 | 0.0s\n",
      "Epoch 429/1000 | train_loss=-0.2780 | 0.0s\n",
      "Epoch 430/1000 | train_loss=-0.2806 | 0.0s\n",
      "Epoch 431/1000 | train_loss=-0.2839 | 0.0s\n",
      "Epoch 432/1000 | train_loss=-0.2794 | 0.0s\n",
      "Epoch 433/1000 | train_loss=-0.2756 | 0.0s\n",
      "Epoch 434/1000 | train_loss=-0.2375 | 0.0s\n",
      "Epoch 435/1000 | train_loss=-0.2766 | 0.0s\n",
      "Epoch 436/1000 | train_loss=-0.2803 | 0.0s\n",
      "Epoch 437/1000 | train_loss=-0.2993 | 0.0s\n",
      "Epoch 438/1000 | train_loss=-0.2995 | 0.0s\n",
      "Epoch 439/1000 | train_loss=-0.2860 | 0.0s\n",
      "Epoch 440/1000 | train_loss=-0.3173 | 0.0s\n",
      "Epoch 441/1000 | train_loss=-0.3061 | 0.0s\n",
      "Epoch 442/1000 | train_loss=-0.2939 | 0.0s\n",
      "Epoch 443/1000 | train_loss=-0.2939 | 0.0s\n",
      "Epoch 444/1000 | train_loss=-0.3188 | 0.0s\n",
      "Epoch 445/1000 | train_loss=-0.2238 | 0.0s\n",
      "Epoch 446/1000 | train_loss=-0.2379 | 0.0s\n",
      "Epoch 447/1000 | train_loss=-0.3040 | 0.0s\n",
      "Epoch 448/1000 | train_loss=-0.2676 | 0.0s\n",
      "Epoch 449/1000 | train_loss=-0.2893 | 0.0s\n",
      "Epoch 450/1000 | train_loss=-0.2705 | 0.0s\n",
      "Epoch 451/1000 | train_loss=-0.3544 | 0.0s\n",
      "Epoch 452/1000 | train_loss=-0.2932 | 0.0s\n",
      "Epoch 453/1000 | train_loss=-0.3621 | 0.0s\n",
      "Epoch 454/1000 | train_loss=-0.2615 | 0.0s\n",
      "Epoch 455/1000 | train_loss=-0.3266 | 0.0s\n",
      "Epoch 456/1000 | train_loss=-0.3098 | 0.0s\n",
      "Epoch 457/1000 | train_loss=-0.3017 | 0.0s\n",
      "Epoch 458/1000 | train_loss=-0.3433 | 0.0s\n",
      "Epoch 459/1000 | train_loss=-0.3608 | 0.0s\n",
      "Epoch 460/1000 | train_loss=-0.2111 | 0.0s\n",
      "Epoch 461/1000 | train_loss=-0.2091 | 0.0s\n",
      "Epoch 462/1000 | train_loss=-0.3293 | 0.0s\n",
      "Epoch 463/1000 | train_loss=-0.2617 | 0.0s\n",
      "Epoch 464/1000 | train_loss=-0.3686 | 0.0s\n",
      "Epoch 465/1000 | train_loss=-0.2446 | 0.0s\n",
      "Epoch 466/1000 | train_loss=-0.2306 | 0.0s\n",
      "Epoch 467/1000 | train_loss=-0.3031 | 0.0s\n",
      "Epoch 468/1000 | train_loss=-0.3166 | 0.0s\n",
      "Epoch 469/1000 | train_loss=-0.3480 | 0.0s\n",
      "Epoch 470/1000 | train_loss=-0.2450 | 0.0s\n",
      "Epoch 471/1000 | train_loss=-0.2851 | 0.0s\n",
      "Epoch 472/1000 | train_loss=-0.3629 | 0.0s\n",
      "Epoch 473/1000 | train_loss=-0.3935 | 0.0s\n",
      "Epoch 474/1000 | train_loss=-0.3878 | 0.0s\n",
      "Epoch 475/1000 | train_loss=-0.3883 | 0.0s\n",
      "Epoch 476/1000 | train_loss=-0.3819 | 0.0s\n",
      "Epoch 477/1000 | train_loss=-0.3838 | 0.0s\n",
      "Epoch 478/1000 | train_loss=-0.4061 | 0.0s\n",
      "Epoch 479/1000 | train_loss=-0.3990 | 0.0s\n",
      "Epoch 480/1000 | train_loss=-0.3863 | 0.0s\n",
      "Epoch 481/1000 | train_loss=-0.4231 | 0.0s\n",
      "Epoch 482/1000 | train_loss=-0.4096 | 0.0s\n",
      "Epoch 483/1000 | train_loss=-0.4122 | 0.0s\n",
      "Epoch 484/1000 | train_loss=-0.4247 | 0.0s\n",
      "Epoch 485/1000 | train_loss=-0.3964 | 0.0s\n",
      "Epoch 486/1000 | train_loss=-0.4047 | 0.0s\n",
      "Epoch 487/1000 | train_loss=-0.4403 | 0.0s\n",
      "Epoch 488/1000 | train_loss=-0.4187 | 0.0s\n",
      "Epoch 489/1000 | train_loss=-0.4291 | 0.0s\n",
      "Epoch 490/1000 | train_loss=-0.3978 | 0.0s\n",
      "Epoch 491/1000 | train_loss=-0.4238 | 0.0s\n",
      "Epoch 492/1000 | train_loss=-0.4234 | 0.0s\n",
      "Epoch 493/1000 | train_loss=-0.4182 | 0.0s\n",
      "Epoch 494/1000 | train_loss=-0.4202 | 0.0s\n",
      "Epoch 495/1000 | train_loss=-0.4454 | 0.0s\n",
      "Epoch 496/1000 | train_loss=-0.4231 | 0.0s\n",
      "Epoch 497/1000 | train_loss=-0.4591 | 0.0s\n",
      "Epoch 498/1000 | train_loss=-0.4725 | 0.0s\n",
      "Epoch 499/1000 | train_loss=-0.4779 | 0.0s\n",
      "Epoch 500/1000 | train_loss=-0.4588 | 0.0s\n",
      "Epoch 501/1000 | train_loss=-0.4853 | 0.0s\n",
      "Epoch 502/1000 | train_loss=-0.4526 | 0.0s\n",
      "Epoch 503/1000 | train_loss=-0.4864 | 0.0s\n",
      "Epoch 504/1000 | train_loss=-0.4760 | 0.0s\n",
      "Epoch 505/1000 | train_loss=-0.5017 | 0.0s\n",
      "Epoch 506/1000 | train_loss=-0.4287 | 0.0s\n",
      "Epoch 507/1000 | train_loss=-0.4846 | 0.0s\n",
      "Epoch 508/1000 | train_loss=-0.4799 | 0.0s\n",
      "Epoch 509/1000 | train_loss=-0.4690 | 0.0s\n",
      "Epoch 510/1000 | train_loss=-0.4826 | 0.0s\n",
      "Epoch 511/1000 | train_loss=-0.4850 | 0.0s\n",
      "Epoch 512/1000 | train_loss=-0.4892 | 0.0s\n",
      "Epoch 513/1000 | train_loss=-0.5241 | 0.0s\n",
      "Epoch 514/1000 | train_loss=-0.5128 | 0.0s\n",
      "Epoch 515/1000 | train_loss=-0.5219 | 0.0s\n",
      "Epoch 516/1000 | train_loss=-0.5079 | 0.0s\n",
      "Epoch 517/1000 | train_loss=-0.5263 | 0.0s\n",
      "Epoch 518/1000 | train_loss=-0.5268 | 0.0s\n",
      "Epoch 519/1000 | train_loss=-0.4988 | 0.0s\n",
      "Epoch 520/1000 | train_loss=-0.5229 | 0.0s\n",
      "Epoch 521/1000 | train_loss=-0.5426 | 0.0s\n",
      "Epoch 522/1000 | train_loss=-0.4879 | 0.0s\n",
      "Epoch 523/1000 | train_loss=-0.5362 | 0.0s\n",
      "Epoch 524/1000 | train_loss=-0.5054 | 0.0s\n",
      "Epoch 525/1000 | train_loss=-0.5268 | 0.0s\n",
      "Epoch 526/1000 | train_loss=-0.5419 | 0.0s\n",
      "Epoch 527/1000 | train_loss=-0.5703 | 0.0s\n",
      "Epoch 528/1000 | train_loss=-0.5502 | 0.0s\n",
      "Epoch 529/1000 | train_loss=-0.5499 | 0.0s\n",
      "Epoch 530/1000 | train_loss=-0.5767 | 0.0s\n",
      "Epoch 531/1000 | train_loss=-0.5851 | 0.0s\n",
      "Epoch 532/1000 | train_loss=-0.5641 | 0.0s\n",
      "Epoch 533/1000 | train_loss=-0.5610 | 0.0s\n",
      "Epoch 534/1000 | train_loss=-0.5759 | 0.0s\n",
      "Epoch 535/1000 | train_loss=-0.5648 | 0.0s\n",
      "Epoch 536/1000 | train_loss=-0.5685 | 0.0s\n",
      "Epoch 537/1000 | train_loss=-0.5739 | 0.0s\n",
      "Epoch 538/1000 | train_loss=-0.5957 | 0.0s\n",
      "Epoch 539/1000 | train_loss=-0.5633 | 0.0s\n",
      "Epoch 540/1000 | train_loss=-0.6016 | 0.0s\n",
      "Epoch 541/1000 | train_loss=-0.5990 | 0.0s\n",
      "Epoch 542/1000 | train_loss=-0.6171 | 0.0s\n",
      "Epoch 543/1000 | train_loss=-0.5966 | 0.0s\n",
      "Epoch 544/1000 | train_loss=-0.5762 | 0.0s\n",
      "Epoch 545/1000 | train_loss=-0.6206 | 0.0s\n",
      "Epoch 546/1000 | train_loss=-0.5947 | 0.0s\n",
      "Epoch 547/1000 | train_loss=-0.6430 | 0.0s\n",
      "Epoch 548/1000 | train_loss=-0.6091 | 0.0s\n",
      "Epoch 549/1000 | train_loss=-0.5900 | 0.0s\n",
      "Epoch 550/1000 | train_loss=-0.6463 | 0.0s\n",
      "Epoch 551/1000 | train_loss=-0.6518 | 0.0s\n",
      "Epoch 552/1000 | train_loss=-0.6207 | 0.0s\n",
      "Epoch 553/1000 | train_loss=-0.6610 | 0.0s\n",
      "Epoch 554/1000 | train_loss=-0.5995 | 0.0s\n",
      "Epoch 555/1000 | train_loss=-0.5678 | 0.0s\n",
      "Epoch 556/1000 | train_loss=-0.6088 | 0.0s\n",
      "Epoch 557/1000 | train_loss=-0.5643 | 0.0s\n",
      "Epoch 558/1000 | train_loss=-0.6099 | 0.0s\n",
      "Epoch 559/1000 | train_loss=-0.5749 | 0.0s\n",
      "Epoch 560/1000 | train_loss=-0.6603 | 0.0s\n",
      "Epoch 561/1000 | train_loss=-0.6761 | 0.0s\n",
      "Epoch 562/1000 | train_loss=-0.6773 | 0.0s\n",
      "Epoch 563/1000 | train_loss=-0.6933 | 0.0s\n",
      "Epoch 564/1000 | train_loss=-0.6828 | 0.0s\n",
      "Epoch 565/1000 | train_loss=-0.6901 | 0.0s\n",
      "Epoch 566/1000 | train_loss=-0.6798 | 0.0s\n",
      "Epoch 567/1000 | train_loss=-0.7028 | 0.0s\n",
      "Epoch 568/1000 | train_loss=-0.6941 | 0.0s\n",
      "Epoch 569/1000 | train_loss=-0.6967 | 0.0s\n",
      "Epoch 570/1000 | train_loss=-0.7127 | 0.0s\n",
      "Epoch 571/1000 | train_loss=-0.7189 | 0.0s\n",
      "Epoch 572/1000 | train_loss=-0.7102 | 0.0s\n",
      "Epoch 573/1000 | train_loss=-0.7156 | 0.0s\n",
      "Epoch 574/1000 | train_loss=-0.6732 | 0.0s\n",
      "Epoch 575/1000 | train_loss=-0.7102 | 0.0s\n",
      "Epoch 576/1000 | train_loss=-0.7204 | 0.0s\n",
      "Epoch 577/1000 | train_loss=-0.7404 | 0.0s\n",
      "Epoch 578/1000 | train_loss=-0.7405 | 0.0s\n",
      "Epoch 579/1000 | train_loss=-0.7403 | 0.0s\n",
      "Epoch 580/1000 | train_loss=-0.6741 | 0.0s\n",
      "Epoch 581/1000 | train_loss=-0.7038 | 0.0s\n",
      "Epoch 582/1000 | train_loss=-0.6794 | 0.0s\n",
      "Epoch 583/1000 | train_loss=-0.7183 | 0.0s\n",
      "Epoch 584/1000 | train_loss=-0.7560 | 0.0s\n",
      "Epoch 585/1000 | train_loss=-0.7759 | 0.0s\n",
      "Epoch 586/1000 | train_loss=-0.7708 | 0.0s\n",
      "Epoch 587/1000 | train_loss=-0.7659 | 0.0s\n",
      "Epoch 588/1000 | train_loss=-0.7665 | 0.0s\n",
      "Epoch 589/1000 | train_loss=-0.7718 | 0.0s\n",
      "Epoch 590/1000 | train_loss=-0.7515 | 0.0s\n",
      "Epoch 591/1000 | train_loss=-0.7438 | 0.0s\n",
      "Epoch 592/1000 | train_loss=-0.7439 | 0.0s\n",
      "Epoch 593/1000 | train_loss=-0.7626 | 0.0s\n",
      "Epoch 594/1000 | train_loss=-0.7733 | 0.0s\n",
      "Epoch 595/1000 | train_loss=-0.7856 | 0.0s\n",
      "Epoch 596/1000 | train_loss=-0.7948 | 0.0s\n",
      "Epoch 597/1000 | train_loss=-0.8110 | 0.0s\n",
      "Epoch 598/1000 | train_loss=-0.7971 | 0.0s\n",
      "Epoch 599/1000 | train_loss=-0.8070 | 0.0s\n",
      "Epoch 600/1000 | train_loss=-0.8127 | 0.0s\n",
      "Epoch 601/1000 | train_loss=-0.8177 | 0.0s\n",
      "Epoch 602/1000 | train_loss=-0.8104 | 0.0s\n",
      "Epoch 603/1000 | train_loss=-0.8019 | 0.0s\n",
      "Epoch 604/1000 | train_loss=-0.8273 | 0.0s\n",
      "Epoch 605/1000 | train_loss=-0.8242 | 0.0s\n",
      "Epoch 606/1000 | train_loss=-0.8417 | 0.0s\n",
      "Epoch 607/1000 | train_loss=-0.7719 | 0.0s\n",
      "Epoch 608/1000 | train_loss=-0.7535 | 0.0s\n",
      "Epoch 609/1000 | train_loss=-0.8438 | 0.0s\n",
      "Epoch 610/1000 | train_loss=-0.8640 | 0.0s\n",
      "Epoch 611/1000 | train_loss=-0.8318 | 0.0s\n",
      "Epoch 612/1000 | train_loss=-0.8125 | 0.0s\n",
      "Epoch 613/1000 | train_loss=-0.8662 | 0.0s\n",
      "Epoch 614/1000 | train_loss=-0.8600 | 0.0s\n",
      "Epoch 615/1000 | train_loss=-0.8619 | 0.0s\n",
      "Epoch 616/1000 | train_loss=-0.8015 | 0.0s\n",
      "Epoch 617/1000 | train_loss=-0.8214 | 0.0s\n",
      "Epoch 618/1000 | train_loss=-0.8721 | 0.0s\n",
      "Epoch 619/1000 | train_loss=-0.8594 | 0.0s\n",
      "Epoch 620/1000 | train_loss=-0.7832 | 0.0s\n",
      "Epoch 621/1000 | train_loss=-0.8105 | 0.0s\n",
      "Epoch 622/1000 | train_loss=-0.8260 | 0.0s\n",
      "Epoch 623/1000 | train_loss=-0.7505 | 0.0s\n",
      "Epoch 624/1000 | train_loss=-0.8652 | 0.0s\n",
      "Epoch 625/1000 | train_loss=-0.8270 | 0.0s\n",
      "Epoch 626/1000 | train_loss=-0.8790 | 0.0s\n",
      "Epoch 627/1000 | train_loss=-0.8993 | 0.0s\n",
      "Epoch 628/1000 | train_loss=-0.8898 | 0.0s\n",
      "Epoch 629/1000 | train_loss=-0.9269 | 0.0s\n",
      "Epoch 630/1000 | train_loss=-0.8529 | 0.0s\n",
      "Epoch 631/1000 | train_loss=-0.8748 | 0.0s\n",
      "Epoch 632/1000 | train_loss=-0.9259 | 0.0s\n",
      "Epoch 633/1000 | train_loss=-0.9229 | 0.0s\n",
      "Epoch 634/1000 | train_loss=-0.9332 | 0.0s\n",
      "Epoch 635/1000 | train_loss=-0.8352 | 0.0s\n",
      "Epoch 636/1000 | train_loss=-0.8855 | 0.0s\n",
      "Epoch 637/1000 | train_loss=-0.9498 | 0.0s\n",
      "Epoch 638/1000 | train_loss=-0.9177 | 0.0s\n",
      "Epoch 639/1000 | train_loss=-0.9232 | 0.0s\n",
      "Epoch 640/1000 | train_loss=-0.9301 | 0.0s\n",
      "Epoch 641/1000 | train_loss=-0.9259 | 0.0s\n",
      "Epoch 642/1000 | train_loss=-0.9559 | 0.0s\n",
      "Epoch 643/1000 | train_loss=-0.9693 | 0.0s\n",
      "Epoch 644/1000 | train_loss=-0.9826 | 0.0s\n",
      "Epoch 645/1000 | train_loss=-0.9472 | 0.0s\n",
      "Epoch 646/1000 | train_loss=-0.9879 | 0.0s\n",
      "Epoch 647/1000 | train_loss=-0.8827 | 0.0s\n",
      "Epoch 648/1000 | train_loss=-0.9731 | 0.0s\n",
      "Epoch 649/1000 | train_loss=-0.9283 | 0.0s\n",
      "Epoch 650/1000 | train_loss=-0.8799 | 0.0s\n",
      "Epoch 651/1000 | train_loss=-1.0288 | 0.0s\n",
      "Epoch 652/1000 | train_loss=-0.8982 | 0.0s\n",
      "Epoch 653/1000 | train_loss=-0.9457 | 0.0s\n",
      "Epoch 654/1000 | train_loss=-1.0067 | 0.0s\n",
      "Epoch 655/1000 | train_loss=-1.0106 | 0.0s\n",
      "Epoch 656/1000 | train_loss=-1.0072 | 0.0s\n",
      "Epoch 657/1000 | train_loss=-1.0085 | 0.0s\n",
      "Epoch 658/1000 | train_loss=-0.9890 | 0.0s\n",
      "Epoch 659/1000 | train_loss=-1.0289 | 0.0s\n",
      "Epoch 660/1000 | train_loss=-1.0277 | 0.0s\n",
      "Epoch 661/1000 | train_loss=-1.0566 | 0.0s\n",
      "Epoch 662/1000 | train_loss=-1.0687 | 0.0s\n",
      "Epoch 663/1000 | train_loss=-0.9773 | 0.0s\n",
      "Epoch 664/1000 | train_loss=-1.0075 | 0.0s\n",
      "Epoch 665/1000 | train_loss=-1.0497 | 0.0s\n",
      "Epoch 666/1000 | train_loss=-0.9887 | 0.0s\n",
      "Epoch 667/1000 | train_loss=-1.0601 | 0.0s\n",
      "Epoch 668/1000 | train_loss=-1.0575 | 0.0s\n",
      "Epoch 669/1000 | train_loss=-1.0911 | 0.0s\n",
      "Epoch 670/1000 | train_loss=-1.0735 | 0.0s\n",
      "Epoch 671/1000 | train_loss=-1.0562 | 0.0s\n",
      "Epoch 672/1000 | train_loss=-1.0958 | 0.0s\n",
      "Epoch 673/1000 | train_loss=-1.0961 | 0.0s\n",
      "Epoch 674/1000 | train_loss=-1.0985 | 0.0s\n",
      "Epoch 675/1000 | train_loss=-1.0967 | 0.0s\n",
      "Epoch 676/1000 | train_loss=-1.1254 | 0.0s\n",
      "Epoch 677/1000 | train_loss=-1.1229 | 0.0s\n",
      "Epoch 678/1000 | train_loss=-1.1383 | 0.0s\n",
      "Epoch 679/1000 | train_loss=-1.1083 | 0.0s\n",
      "Epoch 680/1000 | train_loss=-1.1210 | 0.0s\n",
      "Epoch 681/1000 | train_loss=-1.0989 | 0.0s\n",
      "Epoch 682/1000 | train_loss=-1.1312 | 0.0s\n",
      "Epoch 683/1000 | train_loss=-1.1194 | 0.0s\n",
      "Epoch 684/1000 | train_loss=-1.1166 | 0.0s\n",
      "Epoch 685/1000 | train_loss=-1.1725 | 0.0s\n",
      "Epoch 686/1000 | train_loss=-1.1191 | 0.0s\n",
      "Epoch 687/1000 | train_loss=-1.1561 | 0.0s\n",
      "Epoch 688/1000 | train_loss=-1.1654 | 0.0s\n",
      "Epoch 689/1000 | train_loss=-1.1327 | 0.0s\n",
      "Epoch 690/1000 | train_loss=-1.1230 | 0.0s\n",
      "Epoch 691/1000 | train_loss=-1.1707 | 0.0s\n",
      "Epoch 692/1000 | train_loss=-1.1341 | 0.0s\n",
      "Epoch 693/1000 | train_loss=-1.1296 | 0.0s\n",
      "Epoch 694/1000 | train_loss=-1.1866 | 0.0s\n",
      "Epoch 695/1000 | train_loss=-1.1310 | 0.0s\n",
      "Epoch 696/1000 | train_loss=-1.1591 | 0.0s\n",
      "Epoch 697/1000 | train_loss=-1.2057 | 0.0s\n",
      "Epoch 698/1000 | train_loss=-1.1852 | 0.0s\n",
      "Epoch 699/1000 | train_loss=-1.2452 | 0.0s\n",
      "Epoch 700/1000 | train_loss=-1.1277 | 0.0s\n",
      "Epoch 701/1000 | train_loss=-1.1606 | 0.0s\n",
      "Epoch 702/1000 | train_loss=-1.2260 | 0.0s\n",
      "Epoch 703/1000 | train_loss=-1.2377 | 0.0s\n",
      "Epoch 704/1000 | train_loss=-1.1891 | 0.0s\n",
      "Epoch 705/1000 | train_loss=-1.2090 | 0.0s\n",
      "Epoch 706/1000 | train_loss=-1.2450 | 0.0s\n",
      "Epoch 707/1000 | train_loss=-1.2450 | 0.0s\n",
      "Epoch 708/1000 | train_loss=-1.2625 | 0.0s\n",
      "Epoch 709/1000 | train_loss=-1.2641 | 0.0s\n",
      "Epoch 710/1000 | train_loss=-1.2099 | 0.0s\n",
      "Epoch 711/1000 | train_loss=-1.2663 | 0.0s\n",
      "Epoch 712/1000 | train_loss=-1.2395 | 0.0s\n",
      "Epoch 713/1000 | train_loss=-1.2413 | 0.0s\n",
      "Epoch 714/1000 | train_loss=-1.1984 | 0.0s\n",
      "Epoch 715/1000 | train_loss=-1.2492 | 0.0s\n",
      "Epoch 716/1000 | train_loss=-1.3127 | 0.0s\n",
      "Epoch 717/1000 | train_loss=-1.2633 | 0.0s\n",
      "Epoch 718/1000 | train_loss=-1.2370 | 0.0s\n",
      "Epoch 719/1000 | train_loss=-1.3554 | 0.0s\n",
      "Epoch 720/1000 | train_loss=-1.1631 | 0.0s\n",
      "Epoch 721/1000 | train_loss=-1.3133 | 0.0s\n",
      "Epoch 722/1000 | train_loss=-1.1957 | 0.0s\n",
      "Epoch 723/1000 | train_loss=-1.2186 | 0.0s\n",
      "Epoch 724/1000 | train_loss=-1.2254 | 0.0s\n",
      "Epoch 725/1000 | train_loss=-1.2701 | 0.0s\n",
      "Epoch 726/1000 | train_loss=-1.3143 | 0.0s\n",
      "Epoch 727/1000 | train_loss=-1.3374 | 0.0s\n",
      "Epoch 728/1000 | train_loss=-1.3202 | 0.0s\n",
      "Epoch 729/1000 | train_loss=-1.2855 | 0.0s\n",
      "Epoch 730/1000 | train_loss=-1.2242 | 0.0s\n",
      "Epoch 731/1000 | train_loss=-1.3373 | 0.0s\n",
      "Epoch 732/1000 | train_loss=-1.3760 | 0.0s\n",
      "Epoch 733/1000 | train_loss=-1.3608 | 0.0s\n",
      "Epoch 734/1000 | train_loss=-1.3907 | 0.0s\n",
      "Epoch 735/1000 | train_loss=-1.3703 | 0.0s\n",
      "Epoch 736/1000 | train_loss=-1.3912 | 0.0s\n",
      "Epoch 737/1000 | train_loss=-1.3856 | 0.0s\n",
      "Epoch 738/1000 | train_loss=-1.4035 | 0.0s\n",
      "Epoch 739/1000 | train_loss=-1.3706 | 0.0s\n",
      "Epoch 740/1000 | train_loss=-1.4043 | 0.0s\n",
      "Epoch 741/1000 | train_loss=-1.2894 | 0.0s\n",
      "Epoch 742/1000 | train_loss=-1.2923 | 0.0s\n",
      "Epoch 743/1000 | train_loss=-1.3124 | 0.0s\n",
      "Epoch 744/1000 | train_loss=-1.3551 | 0.0s\n",
      "Epoch 745/1000 | train_loss=-1.3813 | 0.0s\n",
      "Epoch 746/1000 | train_loss=-1.3905 | 0.0s\n",
      "Epoch 747/1000 | train_loss=-1.3466 | 0.0s\n",
      "Epoch 748/1000 | train_loss=-1.2660 | 0.0s\n",
      "Epoch 749/1000 | train_loss=-1.3238 | 0.0s\n",
      "Epoch 750/1000 | train_loss=-1.3924 | 0.0s\n",
      "Epoch 751/1000 | train_loss=-1.4452 | 0.0s\n",
      "Epoch 752/1000 | train_loss=-1.4454 | 0.0s\n",
      "Epoch 753/1000 | train_loss=-1.4609 | 0.0s\n",
      "Epoch 754/1000 | train_loss=-1.4499 | 0.0s\n",
      "Epoch 755/1000 | train_loss=-1.4168 | 0.0s\n",
      "Epoch 756/1000 | train_loss=-1.4616 | 0.0s\n",
      "Epoch 757/1000 | train_loss=-1.4598 | 0.0s\n",
      "Epoch 758/1000 | train_loss=-1.4344 | 0.0s\n",
      "Epoch 759/1000 | train_loss=-1.4784 | 0.0s\n",
      "Epoch 760/1000 | train_loss=-1.4701 | 0.0s\n",
      "Epoch 761/1000 | train_loss=-1.4849 | 0.0s\n",
      "Epoch 762/1000 | train_loss=-1.5123 | 0.0s\n",
      "Epoch 763/1000 | train_loss=-1.5160 | 0.0s\n",
      "Epoch 764/1000 | train_loss=-1.5127 | 0.0s\n",
      "Epoch 765/1000 | train_loss=-1.5324 | 0.0s\n",
      "Epoch 766/1000 | train_loss=-1.5027 | 0.0s\n",
      "Epoch 767/1000 | train_loss=-1.5432 | 0.0s\n",
      "Epoch 768/1000 | train_loss=-1.5183 | 0.0s\n",
      "Epoch 769/1000 | train_loss=-1.5536 | 0.0s\n",
      "Epoch 770/1000 | train_loss=-1.5531 | 0.0s\n",
      "Epoch 771/1000 | train_loss=-1.5478 | 0.0s\n",
      "Epoch 772/1000 | train_loss=-1.5805 | 0.0s\n",
      "Epoch 773/1000 | train_loss=-1.5528 | 0.0s\n",
      "Epoch 774/1000 | train_loss=-1.5732 | 0.0s\n",
      "Epoch 775/1000 | train_loss=-1.5515 | 0.0s\n",
      "Epoch 776/1000 | train_loss=-1.5286 | 0.0s\n",
      "Epoch 777/1000 | train_loss=-1.5886 | 0.0s\n",
      "Epoch 778/1000 | train_loss=-1.5454 | 0.0s\n",
      "Epoch 779/1000 | train_loss=-1.4739 | 0.0s\n",
      "Epoch 780/1000 | train_loss=-1.5655 | 0.0s\n",
      "Epoch 781/1000 | train_loss=-1.5513 | 0.0s\n",
      "Epoch 782/1000 | train_loss=-1.5249 | 0.0s\n",
      "Epoch 783/1000 | train_loss=-1.5831 | 0.0s\n",
      "Epoch 784/1000 | train_loss=-1.6304 | 0.0s\n",
      "Epoch 785/1000 | train_loss=-1.6439 | 0.0s\n",
      "Epoch 786/1000 | train_loss=-1.6073 | 0.0s\n",
      "Epoch 787/1000 | train_loss=-1.6000 | 0.0s\n",
      "Epoch 788/1000 | train_loss=-1.5381 | 0.0s\n",
      "Epoch 789/1000 | train_loss=-1.5850 | 0.0s\n",
      "Epoch 790/1000 | train_loss=-1.6437 | 0.0s\n",
      "Epoch 791/1000 | train_loss=-1.6719 | 0.0s\n",
      "Epoch 792/1000 | train_loss=-1.6576 | 0.0s\n",
      "Epoch 793/1000 | train_loss=-1.6559 | 0.0s\n",
      "Epoch 794/1000 | train_loss=-1.6867 | 0.0s\n",
      "Epoch 795/1000 | train_loss=-1.6292 | 0.0s\n",
      "Epoch 796/1000 | train_loss=-1.6855 | 0.0s\n",
      "Epoch 797/1000 | train_loss=-1.6246 | 0.0s\n",
      "Epoch 798/1000 | train_loss=-1.6798 | 0.0s\n",
      "Epoch 799/1000 | train_loss=-1.6702 | 0.0s\n",
      "Epoch 800/1000 | train_loss=-1.6709 | 0.0s\n",
      "Epoch 801/1000 | train_loss=-1.6644 | 0.0s\n",
      "Epoch 802/1000 | train_loss=-1.7234 | 0.0s\n",
      "Epoch 803/1000 | train_loss=-1.6908 | 0.0s\n",
      "Epoch 804/1000 | train_loss=-1.6931 | 0.0s\n",
      "Epoch 805/1000 | train_loss=-1.6596 | 0.0s\n",
      "Epoch 806/1000 | train_loss=-1.6476 | 0.0s\n",
      "Epoch 807/1000 | train_loss=-1.6923 | 0.0s\n",
      "Epoch 808/1000 | train_loss=-1.7488 | 0.0s\n",
      "Epoch 809/1000 | train_loss=-1.7352 | 0.0s\n",
      "Epoch 810/1000 | train_loss=-1.7615 | 0.0s\n",
      "Epoch 811/1000 | train_loss=-1.7765 | 0.0s\n",
      "Epoch 812/1000 | train_loss=-1.7579 | 0.0s\n",
      "Epoch 813/1000 | train_loss=-1.7547 | 0.0s\n",
      "Epoch 814/1000 | train_loss=-1.7166 | 0.0s\n",
      "Epoch 815/1000 | train_loss=-1.7969 | 0.0s\n",
      "Epoch 816/1000 | train_loss=-1.7033 | 0.0s\n",
      "Epoch 817/1000 | train_loss=-1.7321 | 0.0s\n",
      "Epoch 818/1000 | train_loss=-1.7754 | 0.0s\n",
      "Epoch 819/1000 | train_loss=-1.7977 | 0.0s\n",
      "Epoch 820/1000 | train_loss=-1.8169 | 0.0s\n",
      "Epoch 821/1000 | train_loss=-1.8177 | 0.0s\n",
      "Epoch 822/1000 | train_loss=-1.8223 | 0.0s\n",
      "Epoch 823/1000 | train_loss=-1.8348 | 0.0s\n",
      "Epoch 824/1000 | train_loss=-1.7301 | 0.0s\n",
      "Epoch 825/1000 | train_loss=-1.7883 | 0.0s\n",
      "Epoch 826/1000 | train_loss=-1.7176 | 0.0s\n",
      "Epoch 827/1000 | train_loss=-1.6359 | 0.0s\n",
      "Epoch 828/1000 | train_loss=-1.7202 | 0.0s\n",
      "Epoch 829/1000 | train_loss=-1.4349 | 0.0s\n",
      "Epoch 830/1000 | train_loss=-1.5700 | 0.0s\n",
      "Epoch 831/1000 | train_loss=-1.7571 | 0.0s\n",
      "Epoch 832/1000 | train_loss=-1.6209 | 0.0s\n",
      "Epoch 833/1000 | train_loss=-1.7810 | 0.0s\n",
      "Epoch 834/1000 | train_loss=-1.7632 | 0.0s\n",
      "Epoch 835/1000 | train_loss=-1.8201 | 0.0s\n",
      "Epoch 836/1000 | train_loss=-1.8288 | 0.0s\n",
      "Epoch 837/1000 | train_loss=-1.8475 | 0.0s\n",
      "Epoch 838/1000 | train_loss=-1.8700 | 0.0s\n",
      "Epoch 839/1000 | train_loss=-1.8300 | 0.0s\n",
      "Epoch 840/1000 | train_loss=-1.8929 | 0.0s\n",
      "Epoch 841/1000 | train_loss=-1.7929 | 0.0s\n",
      "Epoch 842/1000 | train_loss=-1.8447 | 0.0s\n",
      "Epoch 843/1000 | train_loss=-1.8136 | 0.0s\n",
      "Epoch 844/1000 | train_loss=-1.8216 | 0.0s\n",
      "Epoch 845/1000 | train_loss=-1.9078 | 0.0s\n",
      "Epoch 846/1000 | train_loss=-1.9266 | 0.0s\n",
      "Epoch 847/1000 | train_loss=-1.8465 | 0.0s\n",
      "Epoch 848/1000 | train_loss=-1.7725 | 0.0s\n",
      "Epoch 849/1000 | train_loss=-1.9159 | 0.0s\n",
      "Epoch 850/1000 | train_loss=-1.9415 | 0.0s\n",
      "Epoch 851/1000 | train_loss=-1.8553 | 0.0s\n",
      "Epoch 852/1000 | train_loss=-1.8981 | 0.0s\n",
      "Epoch 853/1000 | train_loss=-1.9433 | 0.0s\n",
      "Epoch 854/1000 | train_loss=-1.9571 | 0.0s\n",
      "Epoch 855/1000 | train_loss=-1.9666 | 0.0s\n",
      "Epoch 856/1000 | train_loss=-1.9718 | 0.0s\n",
      "Epoch 857/1000 | train_loss=-1.9811 | 0.0s\n",
      "Epoch 858/1000 | train_loss=-1.9828 | 0.0s\n",
      "Epoch 859/1000 | train_loss=-1.8954 | 0.0s\n",
      "Epoch 860/1000 | train_loss=-2.0148 | 0.0s\n",
      "Epoch 861/1000 | train_loss=-1.8720 | 0.0s\n",
      "Epoch 862/1000 | train_loss=-1.9411 | 0.0s\n",
      "Epoch 863/1000 | train_loss=-1.9843 | 0.0s\n",
      "Epoch 864/1000 | train_loss=-2.0275 | 0.0s\n",
      "Epoch 865/1000 | train_loss=-2.0151 | 0.0s\n",
      "Epoch 866/1000 | train_loss=-2.0265 | 0.0s\n",
      "Epoch 867/1000 | train_loss=-1.9977 | 0.0s\n",
      "Epoch 868/1000 | train_loss=-1.7998 | 0.0s\n",
      "Epoch 869/1000 | train_loss=-1.9768 | 0.0s\n",
      "Epoch 870/1000 | train_loss=-2.0323 | 0.0s\n",
      "Epoch 871/1000 | train_loss=-1.9810 | 0.0s\n",
      "Epoch 872/1000 | train_loss=-2.0221 | 0.0s\n",
      "Epoch 873/1000 | train_loss=-1.9867 | 0.0s\n",
      "Epoch 874/1000 | train_loss=-2.0880 | 0.0s\n",
      "Epoch 875/1000 | train_loss=-2.0668 | 0.0s\n",
      "Epoch 876/1000 | train_loss=-2.0746 | 0.0s\n",
      "Epoch 877/1000 | train_loss=-2.0451 | 0.0s\n",
      "Epoch 878/1000 | train_loss=-2.0838 | 0.0s\n",
      "Epoch 879/1000 | train_loss=-2.0476 | 0.0s\n",
      "Epoch 880/1000 | train_loss=-2.0985 | 0.0s\n",
      "Epoch 881/1000 | train_loss=-2.0855 | 0.0s\n",
      "Epoch 882/1000 | train_loss=-2.0808 | 0.0s\n",
      "Epoch 883/1000 | train_loss=-2.0806 | 0.0s\n",
      "Epoch 884/1000 | train_loss=-2.0931 | 0.0s\n",
      "Epoch 885/1000 | train_loss=-2.0856 | 0.0s\n",
      "Epoch 886/1000 | train_loss=-2.0804 | 0.0s\n",
      "Epoch 887/1000 | train_loss=-2.1411 | 0.0s\n",
      "Epoch 888/1000 | train_loss=-2.1286 | 0.0s\n",
      "Epoch 889/1000 | train_loss=-2.1698 | 0.0s\n",
      "Epoch 890/1000 | train_loss=-2.1424 | 0.0s\n",
      "Epoch 891/1000 | train_loss=-2.1453 | 0.0s\n",
      "Epoch 892/1000 | train_loss=-2.1585 | 0.0s\n",
      "Epoch 893/1000 | train_loss=-2.1855 | 0.0s\n",
      "Epoch 894/1000 | train_loss=-2.1780 | 0.0s\n",
      "Epoch 895/1000 | train_loss=-2.1464 | 0.0s\n",
      "Epoch 896/1000 | train_loss=-2.1852 | 0.0s\n",
      "Epoch 897/1000 | train_loss=-2.0858 | 0.0s\n",
      "Epoch 898/1000 | train_loss=-2.2522 | 0.0s\n",
      "Epoch 899/1000 | train_loss=-2.1348 | 0.0s\n",
      "Epoch 900/1000 | train_loss=-2.2332 | 0.0s\n",
      "Epoch 901/1000 | train_loss=-1.9768 | 0.0s\n",
      "Epoch 902/1000 | train_loss=-1.9008 | 0.0s\n",
      "Epoch 903/1000 | train_loss=-2.1925 | 0.0s\n",
      "Epoch 904/1000 | train_loss=-2.1520 | 0.0s\n",
      "Epoch 905/1000 | train_loss=-2.2309 | 0.0s\n",
      "Epoch 906/1000 | train_loss=-2.0213 | 0.0s\n",
      "Epoch 907/1000 | train_loss=-2.1157 | 0.0s\n",
      "Epoch 908/1000 | train_loss=-2.2588 | 0.0s\n",
      "Epoch 909/1000 | train_loss=-2.0298 | 0.0s\n",
      "Epoch 910/1000 | train_loss=-2.1333 | 0.0s\n",
      "Epoch 911/1000 | train_loss=-2.2155 | 0.0s\n",
      "Epoch 912/1000 | train_loss=-2.2366 | 0.0s\n",
      "Epoch 913/1000 | train_loss=-2.2141 | 0.0s\n",
      "Epoch 914/1000 | train_loss=-2.2689 | 0.0s\n",
      "Epoch 915/1000 | train_loss=-2.2882 | 0.0s\n",
      "Epoch 916/1000 | train_loss=-2.2825 | 0.0s\n",
      "Epoch 917/1000 | train_loss=-2.3281 | 0.0s\n",
      "Epoch 918/1000 | train_loss=-2.2488 | 0.0s\n",
      "Epoch 919/1000 | train_loss=-2.3407 | 0.0s\n",
      "Epoch 920/1000 | train_loss=-2.2642 | 0.0s\n",
      "Epoch 921/1000 | train_loss=-2.2974 | 0.0s\n",
      "Epoch 922/1000 | train_loss=-2.3519 | 0.0s\n",
      "Epoch 923/1000 | train_loss=-2.3441 | 0.0s\n",
      "Epoch 924/1000 | train_loss=-2.3669 | 0.0s\n",
      "Epoch 925/1000 | train_loss=-2.3054 | 0.0s\n",
      "Epoch 926/1000 | train_loss=-2.3490 | 0.0s\n",
      "Epoch 927/1000 | train_loss=-2.3823 | 0.0s\n",
      "Epoch 928/1000 | train_loss=-2.3807 | 0.0s\n",
      "Epoch 929/1000 | train_loss=-2.3229 | 0.0s\n",
      "Epoch 930/1000 | train_loss=-2.3239 | 0.0s\n",
      "Epoch 931/1000 | train_loss=-2.3694 | 0.0s\n",
      "Epoch 932/1000 | train_loss=-2.3232 | 0.0s\n",
      "Epoch 933/1000 | train_loss=-2.3658 | 0.0s\n",
      "Epoch 934/1000 | train_loss=-2.3878 | 0.0s\n",
      "Epoch 935/1000 | train_loss=-2.4350 | 0.0s\n",
      "Epoch 936/1000 | train_loss=-2.4258 | 0.0s\n",
      "Epoch 937/1000 | train_loss=-2.4280 | 0.0s\n",
      "Epoch 938/1000 | train_loss=-2.3906 | 0.0s\n",
      "Epoch 939/1000 | train_loss=-2.4084 | 0.0s\n",
      "Epoch 940/1000 | train_loss=-2.4145 | 0.0s\n",
      "Epoch 941/1000 | train_loss=-2.4673 | 0.0s\n",
      "Epoch 942/1000 | train_loss=-2.4747 | 0.0s\n",
      "Epoch 943/1000 | train_loss=-2.4632 | 0.0s\n",
      "Epoch 944/1000 | train_loss=-2.4632 | 0.0s\n",
      "Epoch 945/1000 | train_loss=-2.4813 | 0.0s\n",
      "Epoch 946/1000 | train_loss=-2.4789 | 0.0s\n",
      "Epoch 947/1000 | train_loss=-2.4709 | 0.0s\n",
      "Epoch 948/1000 | train_loss=-2.5036 | 0.0s\n",
      "Epoch 949/1000 | train_loss=-2.5235 | 0.0s\n",
      "Epoch 950/1000 | train_loss=-2.5225 | 0.0s\n",
      "Epoch 951/1000 | train_loss=-2.5463 | 0.0s\n",
      "Epoch 952/1000 | train_loss=-2.5484 | 0.0s\n",
      "Epoch 953/1000 | train_loss=-2.5554 | 0.0s\n",
      "Epoch 954/1000 | train_loss=-2.4745 | 0.0s\n",
      "Epoch 955/1000 | train_loss=-2.5118 | 0.0s\n",
      "Epoch 956/1000 | train_loss=-2.2751 | 0.0s\n",
      "Epoch 957/1000 | train_loss=-2.4973 | 0.0s\n",
      "Epoch 958/1000 | train_loss=-2.2997 | 0.0s\n",
      "Epoch 959/1000 | train_loss=-2.3422 | 0.0s\n",
      "Epoch 960/1000 | train_loss=-2.5223 | 0.0s\n",
      "Epoch 961/1000 | train_loss=-2.3518 | 0.0s\n",
      "Epoch 962/1000 | train_loss=-2.5470 | 0.0s\n",
      "Epoch 963/1000 | train_loss=-2.5774 | 0.0s\n",
      "Epoch 964/1000 | train_loss=-2.5917 | 0.0s\n",
      "Epoch 965/1000 | train_loss=-2.5660 | 0.0s\n",
      "Epoch 966/1000 | train_loss=-2.5342 | 0.0s\n",
      "Epoch 967/1000 | train_loss=-2.5604 | 0.0s\n",
      "Epoch 968/1000 | train_loss=-2.6045 | 0.0s\n",
      "Epoch 969/1000 | train_loss=-2.5781 | 0.0s\n",
      "Epoch 970/1000 | train_loss=-2.5518 | 0.0s\n",
      "Epoch 971/1000 | train_loss=-2.6359 | 0.0s\n",
      "Epoch 972/1000 | train_loss=-2.6684 | 0.0s\n",
      "Epoch 973/1000 | train_loss=-2.5695 | 0.0s\n",
      "Epoch 974/1000 | train_loss=-2.6138 | 0.0s\n",
      "Epoch 975/1000 | train_loss=-2.6439 | 0.0s\n",
      "Epoch 976/1000 | train_loss=-2.6355 | 0.0s\n",
      "Epoch 977/1000 | train_loss=-2.5668 | 0.0s\n",
      "Epoch 978/1000 | train_loss=-2.6216 | 0.0s\n",
      "Epoch 979/1000 | train_loss=-2.6737 | 0.0s\n",
      "Epoch 980/1000 | train_loss=-2.7734 | 0.0s\n",
      "Epoch 981/1000 | train_loss=-2.5139 | 0.0s\n",
      "Epoch 982/1000 | train_loss=-2.5743 | 0.0s\n",
      "Epoch 983/1000 | train_loss=-2.7043 | 0.0s\n",
      "Epoch 984/1000 | train_loss=-2.7121 | 0.0s\n",
      "Epoch 985/1000 | train_loss=-2.7229 | 0.0s\n",
      "Epoch 986/1000 | train_loss=-2.7359 | 0.0s\n",
      "Epoch 987/1000 | train_loss=-2.7523 | 0.0s\n",
      "Epoch 988/1000 | train_loss=-2.7798 | 0.0s\n",
      "Epoch 989/1000 | train_loss=-2.7714 | 0.0s\n",
      "Epoch 990/1000 | train_loss=-2.7144 | 0.0s\n",
      "Epoch 991/1000 | train_loss=-2.7450 | 0.0s\n",
      "Epoch 992/1000 | train_loss=-2.6948 | 0.0s\n",
      "Epoch 993/1000 | train_loss=-2.7589 | 0.0s\n",
      "Epoch 994/1000 | train_loss=-2.7704 | 0.0s\n",
      "Epoch 995/1000 | train_loss=-2.7468 | 0.0s\n",
      "Epoch 996/1000 | train_loss=-2.7010 | 0.0s\n",
      "Epoch 997/1000 | train_loss=-2.7758 | 0.0s\n",
      "Epoch 998/1000 | train_loss=-2.7856 | 0.0s\n",
      "Epoch 999/1000 | train_loss=-2.8650 | 0.0s\n",
      "Epoch 1000/1000 | train_loss=-2.8432 | 0.0s\n",
      "Saved: checkpoints/model_final.pt\n"
     ]
    }
   ],
   "source": [
    "# train loop: train + save checkpoints\n",
    "num_epochs = 1000\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_epoch(model, dl_train, optimizer, criterion)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{num_epochs} | train_loss={tr_loss:.4f} | {dt:.1f}s\")\n",
    "\n",
    "# Save final checkpoint\n",
    "ckpt_path = os.path.join(save_dir, \"model_final.pt\")\n",
    "torch.save({\"model_state\": model.state_dict(), \"config\": config}, ckpt_path)\n",
    "print(\"Saved:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c31968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test preds: 72\n"
     ]
    }
   ],
   "source": [
    "# create test set predictions for eval\n",
    "@torch.no_grad()\n",
    "def predict_on_loader(model, dloader):\n",
    "    model.eval()\n",
    "    out = []  # list of dicts: {id, logit, prob}\n",
    "    for xg, xl, xt, y, ids in dloader:\n",
    "        xg = xg.to(device); xl = xl.to(device)\n",
    "        xt = None if xt is None else xt.to(device).float()\n",
    "        logits = model(xg, xl, xt).squeeze(1)          # [B]\n",
    "        probs  = torch.sigmoid(logits)\n",
    "        for _id, lo, pr in zip(ids, logits.cpu().tolist(), probs.cpu().tolist()):\n",
    "            out.append({\"id\": _id, \"logit\": float(lo), \"prob\": float(pr)})\n",
    "    return out\n",
    "\n",
    "test_preds = predict_on_loader(model, dl_test)\n",
    "print(\"Test preds:\", len(test_preds))\n",
    "\n",
    "# save it as csv\n",
    "# import csv\n",
    "# pred_csv = os.path.join(save_dir, \"test_predictions.csv\")\n",
    "# with open(pred_csv, \"w\", newline=\"\") as f:\n",
    "#     w = csv.DictWriter(f, fieldnames=[\"id\",\"logit\",\"prob\"])\n",
    "#     w.writeheader()\n",
    "#     w.writerows(test_preds)\n",
    "# print(\"Saved:\", pred_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce2afd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N test: 0 | Positives: 0 | Negatives: 0\n"
     ]
    }
   ],
   "source": [
    "## EVAL\n",
    "\n",
    "# get ground truth labels for test ids\n",
    "labels_by_id = {}\n",
    "for _xg, _xl, _xt, _y, _ids in dl_test:\n",
    "    for i, y in zip(_ids, _y.tolist()):\n",
    "        labels_by_id[i] = int(y)\n",
    "len(labels_by_id), list(list(labels_by_id.items())[:3])\n",
    "\n",
    "# get predictions for labels\n",
    "ids  = [d[\"id\"] for d in test_preds if d[\"id\"] in labels_by_id]\n",
    "probs = [d[\"prob\"] for d in test_preds if d[\"id\"] in labels_by_id]\n",
    "labels = [labels_by_id[i] for i in ids]\n",
    "\n",
    "print(f\"N test: {len(labels)} | Positives: {sum(labels)} | Negatives: {len(labels)-sum(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70d80008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: nan\n",
      "Confusion Matrix @ 0.5\n",
      "TP: 0  FP: 0\n",
      "FN: 0  TN: 0\n",
      "Acc: 0.000  Precision: 0.000  Recall: 0.000  F1: 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def metrics_at_threshold(y, p, thr=0.5): #eval metrics at some thresh\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    pred = (p >= thr).astype(int)\n",
    "    tp = int(((pred==1)&(y==1)).sum())\n",
    "    fp = int(((pred==1)&(y==0)).sum())\n",
    "    tn = int(((pred==0)&(y==0)).sum())\n",
    "    fn = int(((pred==0)&(y==1)).sum())\n",
    "    acc  = (tp+tn)/max(len(y),1)\n",
    "    prec = tp/max(tp+fp,1) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/max(tp+fn,1) if (tp+fn)>0 else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return dict(threshold=thr, tp=tp, fp=fp, tn=tn, fn=fn, acc=acc, precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def roc_auc(y, p):# rank based auc\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    n_pos = (y==1).sum(); n_neg = (y==0).sum()\n",
    "    if n_pos==0 or n_neg==0: return float(\"nan\")\n",
    "    order = np.argsort(p)\n",
    "    ranks = np.empty_like(order); ranks[order] = np.arange(1, len(p)+1)\n",
    "    sum_ranks_pos = ranks[y==1].sum()\n",
    "    return float((sum_ranks_pos - n_pos*(n_pos+1)/2) / (n_pos*n_neg))\n",
    "\n",
    "\n",
    "# get metrics / rank + print\n",
    "m = metrics_at_threshold(labels, probs, thr=0.5)\n",
    "auc = roc_auc(labels, probs)\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(\"Confusion Matrix @ 0.5\")\n",
    "print(f\"TP: {m['tp']}  FP: {m['fp']}\")\n",
    "print(f\"FN: {m['fn']}  TN: {m['tn']}\")\n",
    "print(f\"Acc: {m['acc']:.3f}  Precision: {m['precision']:.3f}  Recall: {m['recall']:.3f}  F1: {m['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce9d2f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best F1 threshold search:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         best \u001b[38;5;241m=\u001b[39m mm\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest F1 threshold search:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  F1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  Prec=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbest\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  Rec=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Show a few confident mistakes (helps sanity-check training)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ids, labels, probs))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'precision'"
     ]
    }
   ],
   "source": [
    "# Best-F1 threshold (quick scan over unique probs)\n",
    "uniq = sorted(set(probs))\n",
    "best = {\"f1\": -1.0, \"threshold\": 0.5}\n",
    "for thr in uniq:\n",
    "    mm = metrics_at_threshold(labels, probs, thr)\n",
    "    if mm[\"f1\"] > best[\"f1\"]:\n",
    "        best = mm\n",
    "print(\"\\nBest F1 threshold search:\")\n",
    "print(f\"thr={best['threshold']:.4f}  F1={best['f1']:.3f}  Prec={best['precision']:.3f}  Rec={best['recall']:.3f}\")\n",
    "\n",
    "# Show a few confident mistakes (helps sanity-check training)\n",
    "arr = list(zip(ids, labels, probs))\n",
    "false_pos = sorted([(i,y,p) for i,y,p in arr if y==0], key=lambda t: -t[2])[:5]\n",
    "false_neg = sorted([(i,y,p) for i,y,p in arr if y==1], key=lambda t:  t[2])[:5]\n",
    "\n",
    "print(\"\\nTop false positives by prob (should look like near-miss/non-planet):\")\n",
    "for i,y,p in false_pos: print(f\"id={i}  label={y}  prob={p:.3f}\")\n",
    "\n",
    "print(\"\\nTop false negatives by prob (missed likely planets):\")\n",
    "for i,y,p in false_neg: print(f\"id={i}  label={y}  prob={p:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TORCH SCRIPT EXPORT FOR PRODUCTION #\n",
    "import copy\n",
    "model.eval()\n",
    "model_cpu = copy.deepcopy(model).cpu()\n",
    "scripted = torch.jit.script(model_cpu)\n",
    "scripted.save(\"model_scripted.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabca8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
