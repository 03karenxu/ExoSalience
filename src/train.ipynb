{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43bf66f6",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b660146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(42); np.random.seed(42)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747e4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn import CNN1DEncoder, AstronetMVP\n",
    "from dataset import ExoplanetDataset, collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2566b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FAKE DATA\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ---------- helpers: signal models ----------\n",
    "\n",
    "def trapezoid_transit(phase, depth_ppm, dur_frac, ingress_frac=0.2, center=0.0):\n",
    "    \"\"\"\n",
    "    Simple trapezoid transit model centered at 'center' (in phase units).\n",
    "    phase: array in [-0.5, 0.5]\n",
    "    depth_ppm: positive number (e.g., 500..10000)\n",
    "    dur_frac: total transit duration as fraction of phase (e.g., ~ 0.01)\n",
    "    ingress_frac: fraction of duration spent in ingress+egress (0..1)\n",
    "    \"\"\"\n",
    "    depth = depth_ppm * 1e-6  # convert ppm to relative flux\n",
    "    half = dur_frac / 2.0\n",
    "    p = phase - center\n",
    "\n",
    "    # define trapezoid edges\n",
    "    flat_half = max(half * (1 - ingress_frac), 0.0)\n",
    "    left_ing_start  = -half\n",
    "    left_ing_end    = -flat_half\n",
    "    right_ing_start =  flat_half\n",
    "    right_ing_end   =  half\n",
    "\n",
    "    # shifted piecewise depth profile\n",
    "    y = np.zeros_like(p)\n",
    "    # flat bottom\n",
    "    mask_flat = (p >= left_ing_end) & (p <= right_ing_start)\n",
    "    y[mask_flat] = -depth\n",
    "    # ingress\n",
    "    mask_ing = (p >= left_ing_start) & (p < left_ing_end)\n",
    "    y[mask_ing] = -depth * ( (p[mask_ing] - left_ing_start) / (left_ing_end - left_ing_start) )\n",
    "    # egress\n",
    "    mask_egr = (p > right_ing_start) & (p <= right_ing_end)\n",
    "    y[mask_egr] = -depth * ( 1 - (p[mask_egr] - right_ing_start) / (right_ing_end - right_ing_start) )\n",
    "    return y\n",
    "\n",
    "def eb_like_signal(phase, depth_ppm_primary, depth_ppm_secondary, dur_frac, v_shape=True):\n",
    "    \"\"\"\n",
    "    Eclipsing-binary-like signal: primary dip at 0, secondary at 0.5 phase.\n",
    "    v_shape=True makes narrower ingress/egress (sharper V).\n",
    "    \"\"\"\n",
    "    ingress_frac = 0.9 if v_shape else 0.3\n",
    "    y1 = trapezoid_transit(phase, depth_ppm_primary, dur_frac, ingress_frac=ingress_frac, center=0.0)\n",
    "    y2 = trapezoid_transit(phase, depth_ppm_secondary, dur_frac, ingress_frac=ingress_frac, center=0.5 if np.max(phase) > 0.25 else -0.5)\n",
    "    return y1 + y2\n",
    "\n",
    "def colored_noise(n, alpha=1.0, scale=1.0, rng=None):\n",
    "    \"\"\"\n",
    "    1/f^alpha-ish noise via FFT shaping. alpha=0 -> white, 1 -> pinkish.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(None if rng is None else rng)\n",
    "    # random phases\n",
    "    rand = rng.normal(size=n) + 1j * rng.normal(size=n)\n",
    "    # frequency bins\n",
    "    freqs = np.fft.rfftfreq(n)\n",
    "    mag = np.ones_like(freqs)\n",
    "    mag[1:] = 1.0 / np.power(freqs[1:], alpha)\n",
    "    spec = np.zeros(n//2 + 1, dtype=np.complex128)\n",
    "    spec.real = rand[:spec.size].real * mag\n",
    "    spec.imag = rand[:spec.size].imag * mag\n",
    "    series = np.fft.irfft(spec, n=n)\n",
    "    series = series / (np.std(series) + 1e-8)\n",
    "    return scale * series\n",
    "\n",
    "# ---------- generator ----------\n",
    "\n",
    "def generate_synthetic_exoplanet_dataset(\n",
    "    N=1024,\n",
    "    Tg=2001, Tl=201,\n",
    "    pos_frac=0.5,\n",
    "    tabular_dim=4,\n",
    "    rng_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      global_curves: torch.float32 [N,1,Tg]\n",
    "      local_curves : torch.float32 [N,1,Tl]\n",
    "      labels       : torch.float32 [N] (0/1)\n",
    "      ids          : list[str] length N\n",
    "      tabular      : torch.float32 [N, F]  (or None if tabular_dim=0)\n",
    "\n",
    "    All curves are normalized around ~1.0 and *folded* into phase space.\n",
    "    The local view is a zoom around phase=0 whose width scales with transit duration.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    # Phase axes\n",
    "    phase_g = np.linspace(-0.5, 0.5, Tg, dtype=np.float64)\n",
    "\n",
    "    global_curves = np.empty((N, 1, Tg), dtype=np.float32)\n",
    "    local_curves  = np.empty((N, 1, Tl), dtype=np.float32)\n",
    "    labels        = np.zeros((N,), dtype=np.float32)\n",
    "    ids           = [f\"synth_{i:06d}\" for i in range(N)]\n",
    "    tabular       = None if tabular_dim == 0 else np.empty((N, tabular_dim), dtype=np.float32)\n",
    "\n",
    "    for i in range(N):\n",
    "        is_pos = (rng.random() < pos_frac)\n",
    "        labels[i] = 1.0 if is_pos else 0.0\n",
    "\n",
    "        # Random astrophysical-ish parameters\n",
    "        period_days   = float(rng.uniform(0.5, 20.0))\n",
    "        depth_ppm     = float(rng.uniform(300, 10000)) if is_pos else float(rng.uniform(200, 15000))\n",
    "        dur_hours     = float(rng.uniform(1.0, 6.0))\n",
    "        dur_frac      = dur_hours / (24.0 * period_days)  # duration as fraction of phase\n",
    "        dur_frac      = float(np.clip(dur_frac, 0.002, 0.05))\n",
    "        ingress_frac  = float(rng.uniform(0.1, 0.3))      # smoother U-shape for planets\n",
    "\n",
    "        # Noise level relative to depth\n",
    "        snr_target    = float(rng.uniform(8.0, 50.0)) if is_pos else float(rng.uniform(3.0, 30.0))\n",
    "        noise_std     = (depth_ppm * 1e-6) / max(snr_target, 1.0)\n",
    "\n",
    "        # Base flux ~1 with colored noise\n",
    "        flux = 1.0 + colored_noise(Tg, alpha=rng.uniform(0.5, 1.2), scale=noise_std, rng=rng)\n",
    "\n",
    "        if is_pos:\n",
    "            # Planet-like: single trapezoid at phase=0\n",
    "            transit = trapezoid_transit(phase_g, depth_ppm, dur_frac, ingress_frac=ingress_frac, center=0.0)\n",
    "            flux = flux + transit\n",
    "        else:\n",
    "            # Negatives: half pure noise, half EB-like\n",
    "            if rng.random() < 0.5:\n",
    "                # EB-like: primary + secondary\n",
    "                depth2 = depth_ppm * rng.uniform(0.3, 0.9)\n",
    "                flux = flux + eb_like_signal(phase_g, depth_ppm, depth2, dur_frac, v_shape=True)\n",
    "            else:\n",
    "                # Just noise / variability (already in 'flux')\n",
    "                pass\n",
    "\n",
    "        # Normalize roughly to median ~1\n",
    "        med = np.median(flux)\n",
    "        if med != 0:\n",
    "            flux = flux / med\n",
    "\n",
    "        # Build local view around phase 0 with width scaled by duration\n",
    "        k = 4.0  # half-width in units of duration (wider => more context)\n",
    "        half_width = max(k * dur_frac, 2.0 / Tg)  # ensure at least a couple points\n",
    "        phase_l = np.linspace(-half_width, +half_width, Tl, dtype=np.float64)\n",
    "        # interpolate global flux onto local phase window\n",
    "        local_flux = np.interp(phase_l, phase_g, flux)\n",
    "\n",
    "        # Store\n",
    "        global_curves[i, 0, :] = flux.astype(np.float32)\n",
    "        local_curves[i, 0, :]  = local_flux.astype(np.float32)\n",
    "\n",
    "        # Tabular features (simple, consistent with your 4-col example)\n",
    "        if tabular is not None:\n",
    "            # [period_days, depth_ppm, dur_hours, T_eff (dummy)]\n",
    "            teff = float(rng.uniform(3000, 7000))\n",
    "            tabular[i, :] = np.array([period_days, depth_ppm, dur_hours, teff], dtype=np.float32)\n",
    "\n",
    "    # Convert to tensors\n",
    "    global_t = torch.from_numpy(global_curves)\n",
    "    local_t  = torch.from_numpy(local_curves)\n",
    "    labels_t = torch.from_numpy(labels)\n",
    "    tab_t    = None if tabular is None else torch.from_numpy(tabular)\n",
    "\n",
    "    return global_t, local_t, labels_t, ids, tab_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### -- - - - -  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6b11b",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb6730",
   "metadata": {},
   "source": [
    "to-do: \n",
    "get:\n",
    "- global curves\n",
    "- local curves\n",
    "- labels\n",
    "- ids\n",
    "- tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7fc468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set\n",
    "\n",
    "# TODO: replace with real data\n",
    "\n",
    "# ds = ExoplanetDataset(\n",
    "#     global_curves = ...,\n",
    "#     local_curves = ...,\n",
    "#     labels = ...,\n",
    "#     ids = ...,\n",
    "#     tabular = ...\n",
    "# )\n",
    "\n",
    "global_t, local_t, labels_t, ids_seq, tab_t = generate_synthetic_exoplanet_dataset(\n",
    "    N=2000, Tg=2001, Tl=201, pos_frac=0.5, tabular_dim=4, rng_seed=123\n",
    ")\n",
    "\n",
    "# Split\n",
    "idx = np.arange(len(labels_t))\n",
    "np.random.default_rng(123).shuffle(idx)\n",
    "split = int(0.8 * len(idx))\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "\n",
    "def take(arr, ix):\n",
    "    return arr[ix] if isinstance(arr, np.ndarray) else arr[torch.as_tensor(ix, dtype=torch.long)] if torch.is_tensor(arr) else [arr[i] for i in ix]\n",
    "\n",
    "global_train, global_test = global_t[train_idx], global_t[test_idx]\n",
    "local_train,  local_test  = local_t[train_idx],  local_t[test_idx]\n",
    "labels_train, labels_test = labels_t[train_idx], labels_t[test_idx]\n",
    "ids_train   = [ids_seq[i] for i in train_idx]\n",
    "ids_test    = [ids_seq[i] for i in test_idx]\n",
    "tab_train   = None if tab_t is None else tab_t[train_idx]\n",
    "tab_test    = None if tab_t is None else tab_t[test_idx]\n",
    "\n",
    "ds_train = ExoplanetDataset(global_train, local_train, labels_train, ids_train, tabular=tab_train)\n",
    "ds_test  = ExoplanetDataset(global_test,  local_test,  labels_test,  ids_test,  tabular=tab_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3327c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split from data set\n",
    "#TODO: use real data\n",
    "# train_size = int(0.8 * len(ds))\n",
    "# test_size = len(ds) - train_size\n",
    "\n",
    "# ds_train, ds_test = random_split(ds, [train_size, test_size], generator = torch.Generator().manual_seed(42))\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size = 64, shuffle=True, collate_fn=collate, drop_last=False)\n",
    "dl_test = DataLoader(ds_test, batch_size = 64, shuffle=True, collate_fn=collate, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e83c727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer, pos weight from train set\n",
    "def compute_pos_weight_from_loader(dloader):\n",
    "    pos = neg = 0\n",
    "    with torch.no_grad():\n",
    "        for _xg, _xl, _xt, _y, _ids in dloader:\n",
    "            pos += (_y == 1).sum().item()\n",
    "            neg += (_y == 0).sum().item()\n",
    "    if pos == 0 or neg == 0: return None\n",
    "    return torch.tensor([neg / max(pos, 1.0)], dtype=torch.float32, device=device)\n",
    "\n",
    "# Infer tabular_dim from one train batch\n",
    "xg0, xl0, xt0, y0, ids0 = next(iter(dl_train))\n",
    "tabular_dim = 0 if xt0 is None else xt0.shape[1]\n",
    "\n",
    "model = AstronetMVP(hidden=64, k_global=7, k_local=5, tabular_dim=tabular_dim).to(device)\n",
    "\n",
    "pos_weight = compute_pos_weight_from_loader(dl_train)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if pos_weight is not None else nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81084630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make & save config / checkpoints dir\n",
    "save_dir = \"checkpoints\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# make & save config\n",
    "config = {\n",
    "    \"hidden\":64,\n",
    "    \"k_global\": 7,\n",
    "    \"k_local\": 5,\n",
    "    \"tabular_dim\": tabular_dim,\n",
    "    \"lr\": 1e-3,\n",
    "    \"wd\": 1e-5,\n",
    "    \"pos_weight\": None if pos_weight is None else float(pos_weight.item())\n",
    "}\n",
    "json.dump(config, open(os.path.join(save_dir, \"config.json\"), \"w\"), indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "454b9498",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "adaptive_avg_pool1d(): argument 'input' (position 1) must be Tensor, not Sequential",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     35\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 36\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     dt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dloader, optim, criterion)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# feed\u001b[39;00m\n\u001b[1;32m     16\u001b[0m xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m xt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xt\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 17\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# actual\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, y)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#compute optim\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/ExoSalience/src/cnn.py:43\u001b[0m, in \u001b[0;36mAstronetMVP.forward\u001b[0;34m(self, x_g, x_l, x_tab)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_g, x_l, x_tab):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# feed global & local\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_g\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_g\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     fl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_l(x_l)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# concat global + local\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/ExoSalience/src/cnn.py:23\u001b[0m, in \u001b[0;36mCNN1DEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     22\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[0;32m---> 23\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "File \u001b[0;32m~/.pyenv/versions/hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/hackathon/lib/python3.10/site-packages/torch/nn/modules/pooling.py:1436\u001b[0m, in \u001b[0;36mAdaptiveAvgPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: adaptive_avg_pool1d(): argument 'input' (position 1) must be Tensor, not Sequential"
     ]
    }
   ],
   "source": [
    "# train loop: train + save checkpoints\n",
    "\n",
    "def train_epoch(model, dloader, optim, criterion):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "\n",
    "    # feed train data\n",
    "    for xg, xl, xt, y, _ids in dloader:\n",
    "\n",
    "        # move to gpu\n",
    "        xg = xg.to(device)\n",
    "        xl = xl.to(device)\n",
    "        y = y.to(device).float().view(-1) # expected\n",
    "\n",
    "        # feed\n",
    "        xt = None if xt is None else xt.to(device).float()\n",
    "        logits = model(xg, xl, xt).squeeze(1) # actual\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        #compute optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total += loss.item() * bs\n",
    "        n+= bs\n",
    "\n",
    "    return total / max(n,1)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_epoch(model, dl_train, optimizer, criterion)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{num_epochs} | train_loss={tr_loss:.4f} | {dt:.1f}s\")\n",
    "\n",
    "# Save final checkpoint\n",
    "ckpt_path = os.path.join(save_dir, \"model_final.pt\")\n",
    "torch.save({\"model_state\": model.state_dict(), \"config\": config}, ckpt_path)\n",
    "print(\"Saved:\", ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test set predictions for eval\n",
    "@torch.no_grad()\n",
    "def predict_on_loader(model, dloader):\n",
    "    model.eval()\n",
    "    out = []  # list of dicts: {id, logit, prob}\n",
    "    for xg, xl, xt, y, ids in dloader:\n",
    "        xg = xg.to(device); xl = xl.to(device)\n",
    "        xt = None if xt is None else xt.to(device).float()\n",
    "        logits = model(xg, xl, xt).squeeze(1)          # [B]\n",
    "        probs  = torch.sigmoid(logits)\n",
    "        for _id, lo, pr in zip(ids, logits.cpu().tolist(), probs.cpu().tolist()):\n",
    "            out.append({\"id\": _id, \"logit\": float(lo), \"prob\": float(pr)})\n",
    "    return out\n",
    "\n",
    "test_preds = predict_on_loader(model, dl_test)\n",
    "print(\"Test preds:\", len(test_preds))\n",
    "\n",
    "# save it as csv\n",
    "# import csv\n",
    "# pred_csv = os.path.join(save_dir, \"test_predictions.csv\")\n",
    "# with open(pred_csv, \"w\", newline=\"\") as f:\n",
    "#     w = csv.DictWriter(f, fieldnames=[\"id\",\"logit\",\"prob\"])\n",
    "#     w.writeheader()\n",
    "#     w.writerows(test_preds)\n",
    "# print(\"Saved:\", pred_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce2afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVAL\n",
    "\n",
    "# get ground truth labels for test ids\n",
    "labels_by_id = {}\n",
    "for _xg, _xl, _xt, _y, _ids in dl_test:\n",
    "    for i, y in zip(_ids, _y.tolist()):\n",
    "        labels_by_id[i] = int(y)\n",
    "len(labels_by_id), list(list(labels_by_id.items())[:3])\n",
    "\n",
    "# get predictions for labels\n",
    "ids  = [d[\"id\"] for d in test_preds if d[\"id\"] in labels_by_id]\n",
    "probs = [d[\"prob\"] for d in test_preds if d[\"id\"] in labels_by_id]\n",
    "labels = [labels_by_id[i] for i in ids]\n",
    "\n",
    "print(f\"N test: {len(labels)} | Positives: {sum(labels)} | Negatives: {len(labels)-sum(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d80008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def metrics_at_threshold(y, p, thr=0.5): #eval metrics at some thresh\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    pred = (p >= thr).astype(int)\n",
    "    tp = int(((pred==1)&(y==1)).sum())\n",
    "    fp = int(((pred==1)&(y==0)).sum())\n",
    "    tn = int(((pred==0)&(y==0)).sum())\n",
    "    fn = int(((pred==0)&(y==1)).sum())\n",
    "    acc  = (tp+tn)/max(len(y),1)\n",
    "    prec = tp/max(tp+fp,1) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp/max(tp+fn,1) if (tp+fn)>0 else 0.0\n",
    "    f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return dict(threshold=thr, tp=tp, fp=fp, tn=tn, fn=fn, acc=acc, precision=prec, recall=rec, f1=f1)\n",
    "\n",
    "def roc_auc(y, p):# rank based auc\n",
    "    y = np.asarray(y, dtype=int)\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    n_pos = (y==1).sum(); n_neg = (y==0).sum()\n",
    "    if n_pos==0 or n_neg==0: return float(\"nan\")\n",
    "    order = np.argsort(p)\n",
    "    ranks = np.empty_like(order); ranks[order] = np.arange(1, len(p)+1)\n",
    "    sum_ranks_pos = ranks[y==1].sum()\n",
    "    return float((sum_ranks_pos - n_pos*(n_pos+1)/2) / (n_pos*n_neg))\n",
    "\n",
    "\n",
    "# get metrics / rank + print\n",
    "m = metrics_at_threshold(labels, probs, thr=0.5)\n",
    "auc = roc_auc(labels, probs)\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(\"Confusion Matrix @ 0.5\")\n",
    "print(f\"TP: {m['tp']}  FP: {m['fp']}\")\n",
    "print(f\"FN: {m['fn']}  TN: {m['tn']}\")\n",
    "print(f\"Acc: {m['acc']:.3f}  Precision: {m['precision']:.3f}  Recall: {m['recall']:.3f}  F1: {m['f1']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
